{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook mostly uses command line commands instead of actual Python code to utilize QIIME 2. In the first line of code cells, you can often see `%%bash`, which is used to denote that the chunk contains commands rather than Python code. \n",
    "\n",
    "QIIME 2 has its own file formats: `.qza` and `.qzv`. You can consider `.qza` as files that contain data and `.qzv` files as visualizations of results. To view results stored in `.qzv` files, open [QIIME View](https://view.qiime2.org) (https://view.qiime2.org) and drag your `.qzv` file into the browser window. The file will load and display results. \n",
    "\n",
    "Make sure you are running this notebook with your QIIME 2 virtual environment. If you have followed QIIME 2's installation guide and installed successfully, you should have a dedicated Anaconda/Miniconda virtual environment for QIIME 2. If you are using this notebook with VSCode, select the environment that corresponds to QIIME 2 on the upper right side. \n",
    "\n",
    "If you have any questions about QIIME 2, they have detailed [documentation](https://docs.qiime2.org) online. Specifically for this notebook (time series analysis), many of the commands use the `longitudinal` module. You can check the tutorial on [longitudinal analysis](https://docs.qiime2.org/2022.11/tutorials/longitudinal/) specifically if you want more details on the commands. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import xlrd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import patsy\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "from scipy.spatial.distance import braycurtis, pdist, squareform\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "from skbio import DistanceMatrix\n",
    "from skbio.stats.ordination import pcoa   \n",
    "import scikit_posthocs\n",
    "import math\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change working directory to your folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Aziz/Documents/ApMa Thesis/2023_03_14_012623KBillcus515F_Raw_Data_UDI/data_files\n"
     ]
    }
   ],
   "source": [
    "workdir='/Users/Aziz/Documents/ApMa Thesis/2023_03_14_012623KBillcus515F_Raw_Data_UDI/data_files' # Set this to your working directory\n",
    "%cd $workdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip fastq Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have files in .fastq format, run this chunk to zip it to .fastq.gz format so that importing is more efficient. \n",
    "\n",
    "If you already have files in .fastq.gz format, skip this chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gzip FastQ/*.fastq "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Manifest File and convert Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to manually create a manifest file, which specifies the file paths of .fastq.gz files for each sample. \n",
    "\n",
    "If you have paired end data, create this file in excel with three columns: \"sampleid\", \"forward-absolute-filepath\", \"reverse-absolute-filepath\". If you have single end data, you only need the first two columns.\n",
    "\n",
    "List the sample names in the \"sampleid\" column, and put the absolute file paths to the sample's forward and reverse reads in the next two columns. Then save the excel file as manifest.csv (make sure it is .csv), and run this chunk. \n",
    "\n",
    "For a reference of what a manifest file looks like, check QIIME 2 [documentation](https://docs.qiime2.org/2022.2/tutorials/importing/#fastq-manifest-formats).\n",
    "\n",
    "Below, there is some code to generate a manifest file using file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_files = os.listdir(workdir)\n",
    "print(list_of_files)\n",
    "if '.DS_Store' in list_of_files:\n",
    "    list_of_files.remove('.DS_Store')\n",
    "if \"manifest.tsv\" in list_of_files:\n",
    "    os.remove(\"manifest.tsv\")\n",
    "#print(list_of_files)\n",
    "separator = \"_S1_\"\n",
    "sample_ids = [filename.split(separator, 1)[0] for filename in list_of_files]\n",
    "sample_ids = [*set(sample_ids)]\n",
    "#print(sample_ids)\n",
    "forward_absolute_files = []\n",
    "reverse_absolute_files = []\n",
    "for id in sample_ids:\n",
    "    #find the forward and reverse for this id\n",
    "    forward_reverse = [filename for filename in list_of_files if id in filename]\n",
    "    #print(forward_reverse)\n",
    "    if \"R1\" in forward_reverse[0]:\n",
    "        forward_absolute_files.append(os.path.join(workdir, forward_reverse[0]))\n",
    "        reverse_absolute_files.append(os.path.join(workdir, forward_reverse[1]))\n",
    "    else:\n",
    "        forward_absolute_files.append(os.path.join(workdir, forward_reverse[1]))\n",
    "        reverse_absolute_files.append(os.path.join(workdir, forward_reverse[0]))\n",
    "\n",
    "manifest_dict = {\"sample-id\": sample_ids, \"forward-absolute-filepath\": forward_absolute_files,\n",
    "                \"reverse-absolute-filepath\": reverse_absolute_files}\n",
    "manifest = pd.DataFrame(data=manifest_dict)\n",
    "manifest.to_csv(\"manifest.tsv\", sep = \"\\t\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest = pd.read_csv(\"manifest.csv\")\n",
    "# manifest = manifest.iloc[:,:3] # This line of code keeps the first three columns, making sure there's no empty columns messing up the manifest. May not be necessary\n",
    "manifest.to_csv(\"manifest.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Sequences and View Quality Information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chunk imports the sequences into QIIME.\n",
    "\n",
    "Make sure the `type` and `input-format` parameters are appropriate for your data. If you have single end data, change `type` to `SampleData[SequencesWithQuality]` and `input-format` to `SingleEndFastqManifestPhred33V2`.\n",
    "\n",
    "Make sure the offset for sequence quality score is 33. You should have this information from the people who conducted the sequencing when you get the sequences back. If the offset is 64 instead of 33, change the `33` to `64` in `input-format`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime tools import --type 'SampleData[PairedEndSequencesWithQuality]' --input-path manifest.tsv --output-path demux.qza --input-format PairedEndFastqManifestPhred33V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next chunk visualizes the quality information. After you run this chunk, you should have a `demux.qzv` file in your folder. Open [QIIME View](https://view.qiime2.org) in your browser and drag `demux.qzv` in. Click on the `Interactive Quality Plot` tab at the upper left to view quality information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime demux summarize \\\n",
    "  --i-data demux.qza \\\n",
    "  --o-visualization demux.qzv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Trim and Denoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two choices of denoising methods within QIIME: DADA2 and Deblur. There is no consensus on which is better, but you should have preferences depending on your data type.\n",
    "\n",
    "DADA2 works natively with paired end data, while Deblur does not. To use with Deblur, paired end sequences must be joined first with `qiime vsearch join-pairs` function (or some other joining action) to become joined sequences. However, parameters for the `vsearch join-pairs` operation are very tricky. They are not easily chosen and may lead to loss of read data. \n",
    "\n",
    "Typically, these two methods will give similar results for weighted analysis, but may have variations in unweighted analysis and alpha diversity metrics. DADA2 usually identifies noticeably more ASVs than Deblur in their output. \n",
    "\n",
    "DADA2 takes significantly longer and more memory to run than Deblur, so if you have huge single end read files, you may want to try Deblur first. \n",
    "\n",
    "Be aware that the outputted file names are different for these two methods in order to distinguish (e.g. dada2-table.qza and deblur-table.qza), so change file names accordingly for later analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DADA2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After viewing quality information in `demux.qzv`, you can determine the truncating spot on the left for forward (`p-trim-left-f`) and reverse (`p-trim-left-r`) reads. `p-trunc-len-f` and `p-trunc-len-r`are for truncating spots on the right for forward and reverse reads. `p-n-threads` determines how many cores of you CPU will be used. If you would like to use all cores, put `0` for this parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime dada2 denoise-paired \\\n",
    "  --i-demultiplexed-seqs demux.qza \\\n",
    "  --p-trim-left-f 0 \\\n",
    "  --p-trim-left-r 0 \\\n",
    "  --p-trunc-len-f 220 \\\n",
    "  --p-trunc-len-r 180 \\\n",
    "  --p-n-threads 8 \\\n",
    "  --o-table dada2-table.qza \\\n",
    "  --o-representative-sequences dada2-rep-seqs.qza \\\n",
    "  --o-denoising-stats dada2-denoising-stats.qza "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime feature-table summarize \\\n",
    "  --i-table dada2-table.qza \\\n",
    "  --o-visualization dada2-table.qzv \\\n",
    "  --m-sample-metadata-file metadata.tsv\n",
    "\n",
    "qiime feature-table tabulate-seqs \\\n",
    "  --i-data dada2-rep-seqs.qza \\\n",
    "  --o-visualization dada2-rep-seqs.qzv\n",
    "\n",
    "qiime metadata tabulate \\\n",
    "  --m-input-file dada2-denoising-stats.qza \\\n",
    "  --o-visualization dada2-denoising-stats.qzv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deblur:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have paired end reads, join them first by running this chunk. \n",
    "\n",
    "For `qiime vsearch join-pairs` parameters, refer to [documentation](https://docs.qiime2.org/2022.2/plugins/available/vsearch/join-pairs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime vsearch join-pairs \\\n",
    "  --i-demultiplexed-seqs demux.qza \\\n",
    "  --p-truncqual 10 \\\n",
    "  --p-threads 0 \\\n",
    "  --o-joined-sequences deblur-demux.qza\n",
    "\n",
    "qiime demux summarize \\\n",
    "  --i-data deblur-demux.qza \\\n",
    "  --o-visualization deblur-demux.qzv  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `qiime deblur denoise-16S` parameters, check quality information in `deblur-demux.qzv`. `p-left-trim-len` is the truncating spot on the left, and `p-trim-length` is the truncating spot on the right. `p-jobs-to-start` is the number of CPU cores being used by this command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime deblur denoise-16S \\\n",
    "    --i-demultiplexed-seqs joined-demux.qza \\\n",
    "    --p-trim-length 240 \\\n",
    "    --p-left-trim-len 13 \\\n",
    "    --p-jobs-to-start 4 \\\n",
    "    --o-table deblur-table.qza \\\n",
    "    --o-representative-sequences deblur-rep-seqs.qza \\\n",
    "    --o-stats deblur-denoising-stats.qza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the Feature Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have obtained feature table and representative sequences, we can further process and clean them. \n",
    "\n",
    "One option is to transform the feature table into relative abundance table. Number of sequence reads may vary greatly across samples/individuals. Transforming the table into relative abundance effectively controls the library size (sum of reads for a sample). Additionally, in some cases microbiome data should be considered compositional and therefore should be represented in percentages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime feature-table relative-frequency \\\n",
    "    --i-table dada2-filtered-table.qza \\\n",
    "    --o-relative-frequency-table dada2-relative-table.qza \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also filter by feature prevalence. In the code below, we filter out features whose sum of frequency across all samples is less than 10, and also features present in fewer than 3 samples. You should choose these parameters after inspecting your `table.qzv` file. If you chose to convert your table to relative abundance already, make sure to pass in the relative table in `i-table`, and change `min-frequency` to something like `0.0001`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime feature-table filter-features \\\n",
    "  --i-table dada2-table.qza \\\n",
    "  --p-min-frequency 10 \\\n",
    "  --p-min-samples 3 \\\n",
    "  --o-filtered-table dada2-filtered-table.qza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collapsing by Taxa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also collapse the feature table to different taxonomic levels, i.e. the columns of your feature table will be genera/families rather than species. `p-level` denotes which taxonomic level you want to collapse to. `7` is the lowest, the species level. `6` is the species level, and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved FeatureTable[Frequency] to: phyla-table.qza\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "qiime taxa collapse \\\n",
    "  --i-table dada2-table.qza \\\n",
    "  --i-taxonomy taxonomy.qza \\\n",
    "  --p-level 2 \\\n",
    "  --o-collapsed-table phyla-table.qza"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to look at the feature table as a tsv (viewable in Excel), you can run the below two commands. The first one converts the `.qza` file to a biom table. Then we can convert the biom table to a tsv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime tools export \\\n",
    "  --input-path dada2-relative-table.qza \\\n",
    "  --output-path dada2-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "biom convert \\\n",
    "  -i dada2-table/feature-table.biom \\\n",
    "  -o dada2-table.tsv --to-tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign Taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform taxonomic assignment. QIIME provides classifiers trained on two gene databases: SILVA and Greengenes. \n",
    "\n",
    "SILVA database has a bigger gene tree, is regularly updated, and probably will take more time to run. Greengenes is more compact, was last updated in 2013 (so maybe relatively out-dated), but sometimes it is considered to be more accurate when dealing with specifically human gut microbiome. \n",
    "\n",
    "Below we download two pre-trained Naive Bayes classifiers provided by QIIME. One is trained on SILVA and the other on Greengenes database. Both of these are trained on sequences from only the V4 region in 16S rRNA sequencing. If your sequences are full-length 16S rRNA reads, then get rid of the `-515-806` part of the download link to download classifiers on the full region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget \\\n",
    "  -O 'Silva-V4-classifier.qza' \\\n",
    "  'https://data.qiime2.org/2020.6/common/silva-138-99-515-806-nb-classifier.qza'\n",
    "\n",
    "#wget \\\n",
    "#  -O 'Greengenes-V4-classifier.qza' \\\n",
    "#  'https://data.qiime2.org/2022.2/common/gg-13-8-99-515-806-nb-classifier.qza'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the classifier to assign taxonomy. The code here uses SILVA database. If you want to use Greengenes, change `i-classifier` to `Greengenes-V4-classifier.qza`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime feature-classifier classify-sklearn \\\n",
    "  --i-classifier Silva-V4-classifier.qza \\\n",
    "  --i-reads dada2-rep-seqs.qza \\\n",
    "  --o-classification taxonomy.qza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the taxonomic classifier is ready, you can prepare data table for different taxa levels. \n",
    "\n",
    "1. Genus Level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime feature-table filter-features \\\n",
    "  --i-table genus-table.qza \\\n",
    "  --p-min-frequency 10 \\\n",
    "  --p-min-samples 3 \\\n",
    "  --o-filtered-table genus-filtered-table.qza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime feature-table relative-frequency \\\n",
    "    --i-table genus-filtered-table.qza \\\n",
    "    --o-relative-frequency-table genus-relative-table.qza \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime tools export \\\n",
    "  --input-path genus-relative-table.qza \\\n",
    "  --output-path genus-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "biom convert \\\n",
    "  -i genus-table/feature-table.biom \\\n",
    "  -o genus-table.tsv --to-tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Family Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime feature-table filter-features \\\n",
    "  --i-table family-table.qza \\\n",
    "  --p-min-frequency 10 \\\n",
    "  --p-min-samples 3 \\\n",
    "  --o-filtered-table family-filtered-table.qza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime feature-table relative-frequency \\\n",
    "    --i-table family-filtered-table.qza \\\n",
    "    --o-relative-frequency-table family-relative-table.qza \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime tools export \\\n",
    "  --input-path family-relative-table.qza \\\n",
    "  --output-path family-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "biom convert \\\n",
    "  -i family-table/feature-table.biom \\\n",
    "  -o family-table.tsv --to-tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Order Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime feature-table filter-features \\\n",
    "  --i-table order-table.qza \\\n",
    "  --p-min-frequency 10 \\\n",
    "  --p-min-samples 3 \\\n",
    "  --o-filtered-table order-filtered-table.qza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime feature-table relative-frequency \\\n",
    "    --i-table order-filtered-table.qza \\\n",
    "    --o-relative-frequency-table order-relative-table.qza \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime tools export \\\n",
    "  --input-path order-relative-table.qza \\\n",
    "  --output-path order-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "biom convert \\\n",
    "  -i order-table/feature-table.biom \\\n",
    "  -o order-table.tsv --to-tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Species Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime feature-table filter-features \\\n",
    "  --i-table species-table.qza \\\n",
    "  --p-min-frequency 10 \\\n",
    "  --p-min-samples 3 \\\n",
    "  --o-filtered-table species-filtered-table.qza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime feature-table relative-frequency \\\n",
    "    --i-table species-filtered-table.qza \\\n",
    "    --o-relative-frequency-table species-relative-table.qza \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime tools export \\\n",
    "  --input-path species-relative-table.qza \\\n",
    "  --output-path species-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "biom convert \\\n",
    "  -i species-table/feature-table.biom \\\n",
    "  -o species-table.tsv --to-tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Phylum Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved FeatureTable[Frequency] to: phyla-filtered-table.qza\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "qiime feature-table filter-features \\\n",
    "  --i-table phyla-table.qza \\\n",
    "  --p-min-frequency 10 \\\n",
    "  --p-min-samples 3 \\\n",
    "  --o-filtered-table phyla-filtered-table.qza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved FeatureTable[RelativeFrequency] to: phyla-relative-table.qza\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "qiime feature-table relative-frequency \\\n",
    "    --i-table phyla-filtered-table.qza \\\n",
    "    --o-relative-frequency-table phyla-relative-table.qza \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported phyla-relative-table.qza as BIOMV210DirFmt to directory phyla-table\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "qiime tools export \\\n",
    "  --input-path phyla-relative-table.qza \\\n",
    "  --output-path phyla-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "biom convert \\\n",
    "  -i phyla-table/feature-table.biom \\\n",
    "  -o phyla-table.tsv --to-tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Phylogenetic Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct a phylogenetic tree based on the representative sequences information. Three algorithms for constructing trees are available in QIIME 2: FastTree, IQ-Tree, and RAxML. FastTree runs the fastest but returns poorest quality. IQ-Tree and RAxML will give somewhat different results, but how exactly they differ varies between datasets. Here the code uses RAxML. If you wish to use any of the other method, simply change `raxml` in the command to `fasttree` or `iqtree`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime phylogeny align-to-tree-mafft-raxml \\\n",
    "    --i-sequences dada2-rep-seqs.qza \\\n",
    "    --p-n-threads 4 \\\n",
    "    --output-dir phylogeny-trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime feature-table filter-seqs \\\n",
    "    --i-data dada2-rep-seqs.qza \\\n",
    "    --i-table dada2-filtered-table.qza \\\n",
    "    --o-filtered-data filtered-rep-seqs.qza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Diversity Metrics\n",
    "\n",
    "1.1 Alpha Diversity (Faith's Phylogenetic Diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SampleData[AlphaDiversity] to: species-faith-table.qza\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "qiime diversity alpha-phylogenetic \\\n",
    "  --i-table dada2-table.qza \\\n",
    "  --i-phylogeny phylogeny-trees/rooted_tree.qza \\\n",
    "  --p-metric faith_pd \\\n",
    "  --o-alpha-diversity species-faith-table.qza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime tools export \\\n",
    "  --input-path species-faith-table.qza \\\n",
    "  --output-path species-faith-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "biom convert \\\n",
    "  -i species-faith-table/feature-table.biom \\\n",
    "  -o  --to-tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Simpson's Alpha Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime diversity alpha \\\n",
    "  --i-table species-filtered-table.qza \\\n",
    "  --p-metric simpson \\\n",
    "  --o-alpha-diversity species-simpson-table.qza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime tools export \\\n",
    "  --input-path species-simpson-table.qza \\\n",
    "  --output-path species-simpson-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "biom convert \\\n",
    "  -i species-simpson-table/feature-table.biom \\\n",
    "  -o  --to-tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Beta Diversity (Unweighted UniFrac -- Phylogenetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DistanceMatrix to: unweighted_unifrac_distance_matrix.qza\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "qiime diversity beta-phylogenetic \\\n",
    "  --i-table dada2-table.qza \\\n",
    "  --i-phylogeny phylogeny-trees/rooted_tree.qza \\\n",
    "  --p-metric unweighted_unifrac \\\n",
    "  --o-distance-matrix unweighted_unifrac_distance_matrix.qza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qiime tools export \\\n",
    "  --input-path unweighted_unifrac_distance_matrix.qza \\\n",
    "  --output-path unweighted_unifrac_distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.11 Processing Beta Diversity Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         UniFrac_Distance\n",
      "01.0805          0.000000\n",
      "01.0806          0.251574\n",
      "01.0807          0.234049\n",
      "01.0808          0.470014\n",
      "01.0809          0.519306\n",
      "...                   ...\n",
      "16.1014          0.438051\n",
      "16.1018          0.427984\n",
      "16.1019          0.507704\n",
      "16.1101          0.277274\n",
      "09.0808          0.339151\n",
      "\n",
      "[247 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "unifrac_distance_matrix = pd.read_csv(\"unweighted_unifrac_distance_matrix/distance-matrix.tsv\",sep=\"\\t\", index_col=0, header=0)\n",
    "unifrac_distance_matrix.drop(axis = 0, index=unifrac_distance_matrix.columns[-5:], inplace = True)\n",
    "unifrac_distance_matrix.drop(axis = 1, columns=unifrac_distance_matrix.columns[-5:], inplace = True)\n",
    "\n",
    "uni_1 = unifrac_distance_matrix.loc[unifrac_distance_matrix.columns.str.startswith(\"01.\"),unifrac_distance_matrix.columns.str.startswith(\"01.\")].iloc[:,0]\n",
    "uni_4 = unifrac_distance_matrix.loc[unifrac_distance_matrix.columns.str.startswith(\"04.\"),unifrac_distance_matrix.columns.str.startswith(\"04.\")].iloc[:,0]\n",
    "uni_5 = unifrac_distance_matrix.loc[unifrac_distance_matrix.columns.str.startswith(\"05.\"),unifrac_distance_matrix.columns.str.startswith(\"05.\")].iloc[:,0]\n",
    "uni_8 = unifrac_distance_matrix.loc[unifrac_distance_matrix.columns.str.startswith(\"08.\"),unifrac_distance_matrix.columns.str.startswith(\"08.\")].iloc[:,0]\n",
    "uni_9 = unifrac_distance_matrix.loc[unifrac_distance_matrix.columns.str.startswith(\"09.\"),unifrac_distance_matrix.columns.str.startswith(\"09.\")].iloc[:,0]\n",
    "uni_16 = unifrac_distance_matrix.loc[unifrac_distance_matrix.columns.str.startswith(\"16.\"),unifrac_distance_matrix.columns.str.startswith(\"16.\")].iloc[:,0]\n",
    "\n",
    "unifrac_distance = pd.concat([uni_1,uni_4, uni_5, uni_8, uni_9, uni_16], axis = 0).to_frame(name = \"UniFrac_Distance\")\n",
    "unifrac_distance.at[\"09.0808\", \"UniFrac_Distance\"] = (unifrac_distance.at[\"09.0808.1\", \"UniFrac_Distance\"] + unifrac_distance.at[\"09.0808.2\", \"UniFrac_Distance\"]) /2\n",
    "\n",
    "print(unifrac_distance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Bray Curtis Dissimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample_ID\n",
      "01.0805    0.000000\n",
      "01.0806    0.389631\n",
      "01.0807    0.337004\n",
      "01.0808    0.461837\n",
      "01.0809    0.450743\n",
      "             ...   \n",
      "16.1014    0.471101\n",
      "16.1018    0.441742\n",
      "16.1019    0.453547\n",
      "16.1101    0.317027\n",
      "09.0808    0.376814\n",
      "Name: BC_Dissimilarity, Length: 247, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "species_table = pd.read_csv(\"species-table.tsv\", sep = \"\\t\", skiprows=[0], index_col = 0)\n",
    "species_table = species_table.T\n",
    "species_table.index.names = [\"Sample_ID\"]\n",
    "species_table.drop(species_table.tail(5).index,inplace=True) # drop last n rows\n",
    "\n",
    "species = species_table.columns\n",
    "\n",
    "def apply_BC(df):\n",
    "    first_row = df.loc[df.index[0], :].values.flatten().tolist()[0:-2]\n",
    "    for index in df.index.values:\n",
    "        row = df.loc[index, :].values.flatten().tolist()[0:-2]\n",
    "        df.at[index, \"BC_Dissimilarity\"] = braycurtis(row, first_row)\n",
    "    return df\n",
    "species_table[\"Player\"] = [i.split(\".\")[0] for i in species_table.index.tolist()]\n",
    "species_table[\"BC_Dissimilarity\"] = np.nan\n",
    "species_table = species_table.groupby(\"Player\").apply(lambda df: apply_BC(df))\n",
    "species_table.at[\"09.0808\", \"BC_Dissimilarity\"] = (species_table.at[\"09.0808.1\", \"BC_Dissimilarity\"] + species_table.at[\"09.0808.2\", \"BC_Dissimilarity\"]) /2\n",
    "print(species_table.loc[:,\"BC_Dissimilarity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Diversity Data, Fecal Sample data, and Survey Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Hour of Fecal Sample (replaced missing values with participants average)  \\\n",
      "Sample_ID                                                                            \n",
      "01.0718                                                  NaN                         \n",
      "01.0719                                                  NaN                         \n",
      "01.0720                                                  NaN                         \n",
      "01.0721                                                  NaN                         \n",
      "01.0722                                                  NaN                         \n",
      "...                                                      ...                         \n",
      "16.1202                                                  NaN                         \n",
      "16.1203                                                  NaN                         \n",
      "16.1204                                                  NaN                         \n",
      "16.1205                                                  NaN                         \n",
      "16.1206                                                  NaN                         \n",
      "\n",
      "          Hour of impact (mid point of practice or game)  Impact load day of  \\\n",
      "Sample_ID                                                                      \n",
      "01.0718                                              NaN                 0.0   \n",
      "01.0719                                              NaN                 0.0   \n",
      "01.0720                                              NaN                 0.0   \n",
      "01.0721                                              NaN                 0.0   \n",
      "01.0722                                              NaN                 0.0   \n",
      "...                                                  ...                 ...   \n",
      "16.1202                                              NaN                 0.0   \n",
      "16.1203                                              NaN                 0.0   \n",
      "16.1204                                              NaN                 0.0   \n",
      "16.1205                                              NaN                 0.0   \n",
      "16.1206                                              NaN                 0.0   \n",
      "\n",
      "           Impact load sustained 0-24 hours prior  \\\n",
      "Sample_ID                                           \n",
      "01.0718                                       0.0   \n",
      "01.0719                                       0.0   \n",
      "01.0720                                       0.0   \n",
      "01.0721                                       0.0   \n",
      "01.0722                                       0.0   \n",
      "...                                           ...   \n",
      "16.1202                                       0.0   \n",
      "16.1203                                       0.0   \n",
      "16.1204                                       0.0   \n",
      "16.1205                                       0.0   \n",
      "16.1206                                       0.0   \n",
      "\n",
      "           Impact load sustained 24-48 hours prior  \\\n",
      "Sample_ID                                            \n",
      "01.0718                                        0.0   \n",
      "01.0719                                        0.0   \n",
      "01.0720                                        0.0   \n",
      "01.0721                                        0.0   \n",
      "01.0722                                        0.0   \n",
      "...                                            ...   \n",
      "16.1202                                        0.0   \n",
      "16.1203                                        0.0   \n",
      "16.1204                                        0.0   \n",
      "16.1205                                        0.0   \n",
      "16.1206                                        0.0   \n",
      "\n",
      "           Impact load sustained 48-72 hours priot  \\\n",
      "Sample_ID                                            \n",
      "01.0718                                        0.0   \n",
      "01.0719                                        0.0   \n",
      "01.0720                                        0.0   \n",
      "01.0721                                        0.0   \n",
      "01.0722                                        0.0   \n",
      "...                                            ...   \n",
      "16.1202                                        0.0   \n",
      "16.1203                                        0.0   \n",
      "16.1204                                        0.0   \n",
      "16.1205                                        0.0   \n",
      "16.1206                                        0.0   \n",
      "\n",
      "           Impact load sustained 72-96 hours priot  \\\n",
      "Sample_ID                                            \n",
      "01.0718                                        0.0   \n",
      "01.0719                                        0.0   \n",
      "01.0720                                        0.0   \n",
      "01.0721                                        0.0   \n",
      "01.0722                                        0.0   \n",
      "...                                            ...   \n",
      "16.1202                                        0.0   \n",
      "16.1203                                        0.0   \n",
      "16.1204                                        0.0   \n",
      "16.1205                                        0.0   \n",
      "16.1206                                        0.0   \n",
      "\n",
      "           Non specific impact load - (Team Average for participants 8 and 16)  \\\n",
      "Sample_ID                                                                        \n",
      "01.0718                                                  0.0                     \n",
      "01.0719                                                  0.0                     \n",
      "01.0720                                                  0.0                     \n",
      "01.0721                                                  0.0                     \n",
      "01.0722                                                  0.0                     \n",
      "...                                                      ...                     \n",
      "16.1202                                                  0.0                     \n",
      "16.1203                                                  0.0                     \n",
      "16.1204                                                  0.0                     \n",
      "16.1205                                                  0.0                     \n",
      "16.1206                                                  0.0                     \n",
      "\n",
      "           Total Player Load (Team Average for 8 and 16) - effort metric  \\\n",
      "Sample_ID                                                                  \n",
      "01.0718                                                  NaN               \n",
      "01.0719                                                  NaN               \n",
      "01.0720                                                  NaN               \n",
      "01.0721                                                  NaN               \n",
      "01.0722                                                  NaN               \n",
      "...                                                      ...               \n",
      "16.1202                                                  NaN               \n",
      "16.1203                                                  NaN               \n",
      "16.1204                                                  NaN               \n",
      "16.1205                                                  NaN               \n",
      "16.1206                                                  NaN               \n",
      "\n",
      "           Bristol Stool Scale  ...  Reaction time (ms)  ImpulseControl (ms)  \\\n",
      "Sample_ID                       ...                                            \n",
      "01.0718                    NaN  ...                 NaN                  NaN   \n",
      "01.0719                    NaN  ...                 NaN                  NaN   \n",
      "01.0720                    NaN  ...                 NaN                  NaN   \n",
      "01.0721                    NaN  ...                 NaN                  NaN   \n",
      "01.0722                    NaN  ...                 NaN                  NaN   \n",
      "...                        ...  ...                 ...                  ...   \n",
      "16.1202                    NaN  ...                 NaN                  NaN   \n",
      "16.1203                    NaN  ...                 NaN                  NaN   \n",
      "16.1204                    NaN  ...                 NaN                  NaN   \n",
      "16.1205                    NaN  ...                 NaN                  NaN   \n",
      "16.1206                    NaN  ...                 NaN                  NaN   \n",
      "\n",
      "           InspectionTime (ms)  faith_pd  UniFrac_Distance  BC_Dissimilarity  \\\n",
      "Sample_ID                                                                      \n",
      "01.0718                    NaN       NaN               NaN               NaN   \n",
      "01.0719                    NaN       NaN               NaN               NaN   \n",
      "01.0720                    NaN       NaN               NaN               NaN   \n",
      "01.0721                    NaN       NaN               NaN               NaN   \n",
      "01.0722                    NaN       NaN               NaN               NaN   \n",
      "...                        ...       ...               ...               ...   \n",
      "16.1202                    NaN       NaN               NaN               NaN   \n",
      "16.1203                    NaN       NaN               NaN               NaN   \n",
      "16.1204                    NaN       NaN               NaN               NaN   \n",
      "16.1205                    NaN       NaN               NaN               NaN   \n",
      "16.1206                    NaN       NaN               NaN               NaN   \n",
      "\n",
      "           simpson  Player  Date  Days  \n",
      "Sample_ID                               \n",
      "01.0718        NaN     NaN   NaN   NaN  \n",
      "01.0719        NaN     NaN   NaN   NaN  \n",
      "01.0720        NaN     NaN   NaN   NaN  \n",
      "01.0721        NaN     NaN   NaN   NaN  \n",
      "01.0722        NaN     NaN   NaN   NaN  \n",
      "...            ...     ...   ...   ...  \n",
      "16.1202        NaN     NaN   NaN   NaN  \n",
      "16.1203        NaN     NaN   NaN   NaN  \n",
      "16.1204        NaN     NaN   NaN   NaN  \n",
      "16.1205        NaN     NaN   NaN   NaN  \n",
      "16.1206        NaN     NaN   NaN   NaN  \n",
      "\n",
      "[857 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "master_data = pd.read_csv(\"../Master Spreadsheet - Master Data Sheet.tsv\",sep = \"\\t\",dtype={\"ID/Study Date \":\"str\"}) \n",
    "master_data = master_data.set_index(master_data.columns[0])\n",
    "\n",
    "#Load Faith's Diversity\n",
    "alpha_diversity_table = pd.read_csv(\"species-faith-table/alpha-diversity.tsv\", sep = \"\\t\",\n",
    "    index_col=0)\n",
    "alpha_diversity_table.drop(alpha_diversity_table.tail(5).index,inplace=True) # drop last n rows\n",
    "alpha_diversity_table.at[\"09.0808\", \"faith_pd\"] = (alpha_diversity_table.at[\"09.0808.1\", \"faith_pd\"] + alpha_diversity_table.at[\"09.0808.2\", \"faith_pd\"]) /2\n",
    "\n",
    "\n",
    "#Load Simpson's Diversity\n",
    "simpson_table = pd.read_csv(\"species-simpson-table/alpha-diversity.tsv\", sep = \"\\t\",\n",
    "    index_col=0)\n",
    "simpson_table.drop(simpson_table.tail(5).index,inplace=True) # drop last n rows\n",
    "simpson_table.at[\"09.0808\", \"simpson\"] = (simpson_table.at[\"09.0808.1\", \"simpson\"] + simpson_table.at[\"09.0808.2\", \"simpson\"]) /2\n",
    "\n",
    "\n",
    "#Combine all four diveristy metrics\n",
    "alpha_diversity_table = alpha_diversity_table.join(unifrac_distance)\n",
    "alpha_diversity_table[\"BC_Dissimilarity\"] = species_table[\"BC_Dissimilarity\"]\n",
    "alpha_diversity_table = alpha_diversity_table.join(simpson_table)\n",
    "\n",
    "#Obtain Days\n",
    "alpha_diversity_table[\"Player\"]= [i.split(\".\")[0] for i in alpha_diversity_table.index.tolist()]\n",
    "\n",
    "alpha_diversity_table[\"Date\"] = [datetime.datetime.strptime(i.split(\".\")[1] + \"2022\", \"%m%d%Y\").date() for i in alpha_diversity_table.index.tolist()]\n",
    "alpha_diversity_table[\"Days\"]=  alpha_diversity_table.loc[:, [\"Date\", \"Player\"]].groupby(\"Player\").apply(lambda x: x.subtract(x.iloc[0]))\n",
    "alpha_diversity_table[\"Days\"] = [i.days for i in alpha_diversity_table[\"Days\"]]\n",
    "\n",
    "alpha_master_data = master_data.join(alpha_diversity_table)\n",
    "alpha_master_data.index.name = \"Sample_ID\"\n",
    "alpha_master_data.to_csv(\"../diversity_master.tsv\", sep =\"\\t\")\n",
    "print(alpha_master_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform same process for different taxa levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_change_manual(df, colnames):\n",
    "    n_rows = df.shape[0]\n",
    "    for i in range(n_rows-1,-1,-1):\n",
    "        for col in colnames:\n",
    "            \n",
    "            if i ==0:\n",
    "                df.at[df.index.values[i], col] = np.nan\n",
    "            elif df.at[df.index.values[i], \"TWO FECAL SAMPLES FOLLOWING HEAD IMPACT?\"] == 1:\n",
    "                ##Two days prior\n",
    "                if np.isnan((df.at[df.index.values[i-2], col])): \n",
    "                    df.at[df.index.values[i], col] = np.nan\n",
    "                else:\n",
    "                    df.at[df.index.values[i], col] = (df.at[df.index.values[i], col] - df.at[df.index.values[i-2], col])/(df.at[df.index.values[i-2], col] + 0.01)\n",
    "            else:\n",
    "                if np.isnan((df.at[df.index.values[i-1], col])):\n",
    "                    df.at[df.index.values[i], col] = np.nan\n",
    "                else:\n",
    "                    df.at[df.index.values[i], col] = (df.at[df.index.values[i], col] - df.at[df.index.values[i-1], col])/(df.at[df.index.values[i-1], col] + 0.01)\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Participant's average collection time (24 hour time rounded to the nearest hour)  \\\n",
      "Sample_ID                                                                                     \n",
      "01.0718                                                   12                                  \n",
      "01.0719                                                   12                                  \n",
      "01.0720                                                   12                                  \n",
      "01.0721                                                   12                                  \n",
      "01.0722                                                   12                                  \n",
      "...                                                      ...                                  \n",
      "16.1202                                                   12                                  \n",
      "16.1203                                                   12                                  \n",
      "16.1204                                                   12                                  \n",
      "16.1205                                                   12                                  \n",
      "16.1206                                                   12                                  \n",
      "\n",
      "          Hour of Fecal Sample (replaced missing values with participants average)  \\\n",
      "Sample_ID                                                                            \n",
      "01.0718                                                  NaN                         \n",
      "01.0719                                                  NaN                         \n",
      "01.0720                                                  NaN                         \n",
      "01.0721                                                  NaN                         \n",
      "01.0722                                                  NaN                         \n",
      "...                                                      ...                         \n",
      "16.1202                                                  NaN                         \n",
      "16.1203                                                  NaN                         \n",
      "16.1204                                                  NaN                         \n",
      "16.1205                                                  NaN                         \n",
      "16.1206                                                  NaN                         \n",
      "\n",
      "          Hour of impact (mid point of practice or game)  NEW IMPACT LOAD  \\\n",
      "Sample_ID                                                                   \n",
      "01.0718                                              NaN              0.0   \n",
      "01.0719                                              NaN              0.0   \n",
      "01.0720                                              NaN              0.0   \n",
      "01.0721                                              NaN              0.0   \n",
      "01.0722                                              NaN              0.0   \n",
      "...                                                  ...              ...   \n",
      "16.1202                                              NaN              0.0   \n",
      "16.1203                                              NaN              0.0   \n",
      "16.1204                                              NaN              0.0   \n",
      "16.1205                                              NaN              0.0   \n",
      "16.1206                                              NaN              0.0   \n",
      "\n",
      "           IMPACT LOAD 2-24 HOURS BEFORE FECAL SAMPLE  \\\n",
      "Sample_ID                                               \n",
      "01.0718                                           NaN   \n",
      "01.0719                                           NaN   \n",
      "01.0720                                           NaN   \n",
      "01.0721                                           NaN   \n",
      "01.0722                                           NaN   \n",
      "...                                               ...   \n",
      "16.1202                                           NaN   \n",
      "16.1203                                           NaN   \n",
      "16.1204                                           NaN   \n",
      "16.1205                                           NaN   \n",
      "16.1206                                           NaN   \n",
      "\n",
      "           TWO FECAL SAMPLES FOLLOWING HEAD IMPACT?  Number of head impacts  \\\n",
      "Sample_ID                                                                     \n",
      "01.0718                                         NaN                       0   \n",
      "01.0719                                         NaN                       0   \n",
      "01.0720                                         NaN                       0   \n",
      "01.0721                                         NaN                       0   \n",
      "01.0722                                         NaN                       0   \n",
      "...                                             ...                     ...   \n",
      "16.1202                                         NaN                       0   \n",
      "16.1203                                         NaN                       0   \n",
      "16.1204                                         NaN                       0   \n",
      "16.1205                                         NaN                       0   \n",
      "16.1206                                         NaN                       0   \n",
      "\n",
      "           Non specific impact load - (Team Average for participants 8 and 16)  \\\n",
      "Sample_ID                                                                        \n",
      "01.0718                                                  0.0                     \n",
      "01.0719                                                  0.0                     \n",
      "01.0720                                                  0.0                     \n",
      "01.0721                                                  0.0                     \n",
      "01.0722                                                  0.0                     \n",
      "...                                                      ...                     \n",
      "16.1202                                                  0.0                     \n",
      "16.1203                                                  0.0                     \n",
      "16.1204                                                  0.0                     \n",
      "16.1205                                                  0.0                     \n",
      "16.1206                                                  0.0                     \n",
      "\n",
      "           Total Player Load (Team Average for 8 and 16) - effort metric  \\\n",
      "Sample_ID                                                                  \n",
      "01.0718                                                  NaN               \n",
      "01.0719                                                  NaN               \n",
      "01.0720                                                  NaN               \n",
      "01.0721                                                  NaN               \n",
      "01.0722                                                  NaN               \n",
      "...                                                      ...               \n",
      "16.1202                                                  NaN               \n",
      "16.1203                                                  NaN               \n",
      "16.1204                                                  NaN               \n",
      "16.1205                                                  NaN               \n",
      "16.1206                                                  NaN               \n",
      "\n",
      "           Bristol Stool Scale  ...  Peptostreptococcales-Tissierellales  \\\n",
      "Sample_ID                       ...                                        \n",
      "01.0718                    NaN  ...                                  NaN   \n",
      "01.0719                    NaN  ...                                  NaN   \n",
      "01.0720                    NaN  ...                                  NaN   \n",
      "01.0721                    NaN  ...                                  NaN   \n",
      "01.0722                    NaN  ...                                  NaN   \n",
      "...                        ...  ...                                  ...   \n",
      "16.1202                    NaN  ...                                  NaN   \n",
      "16.1203                    NaN  ...                                  NaN   \n",
      "16.1204                    NaN  ...                                  NaN   \n",
      "16.1205                    NaN  ...                                  NaN   \n",
      "16.1206                    NaN  ...                                  NaN   \n",
      "\n",
      "           Acidaminococcales  Veillonellales-Selenomonadales  \\\n",
      "Sample_ID                                                      \n",
      "01.0718                  NaN                             NaN   \n",
      "01.0719                  NaN                             NaN   \n",
      "01.0720                  NaN                             NaN   \n",
      "01.0721                  NaN                             NaN   \n",
      "01.0722                  NaN                             NaN   \n",
      "...                      ...                             ...   \n",
      "16.1202                  NaN                             NaN   \n",
      "16.1203                  NaN                             NaN   \n",
      "16.1204                  NaN                             NaN   \n",
      "16.1205                  NaN                             NaN   \n",
      "16.1206                  NaN                             NaN   \n",
      "\n",
      "           Rhodospirillales  Burkholderiales  Enterobacterales  \\\n",
      "Sample_ID                                                        \n",
      "01.0718                 NaN              NaN               NaN   \n",
      "01.0719                 NaN              NaN               NaN   \n",
      "01.0720                 NaN              NaN               NaN   \n",
      "01.0721                 NaN              NaN               NaN   \n",
      "01.0722                 NaN              NaN               NaN   \n",
      "...                     ...              ...               ...   \n",
      "16.1202                 NaN              NaN               NaN   \n",
      "16.1203                 NaN              NaN               NaN   \n",
      "16.1204                 NaN              NaN               NaN   \n",
      "16.1205                 NaN              NaN               NaN   \n",
      "16.1206                 NaN              NaN               NaN   \n",
      "\n",
      "           Pasteurellales  Pseudomonadales  Verrucomicrobiales  Player  \n",
      "Sample_ID                                                               \n",
      "01.0718               NaN              NaN                 NaN      01  \n",
      "01.0719               NaN              NaN                 NaN      01  \n",
      "01.0720               NaN              NaN                 NaN      01  \n",
      "01.0721               NaN              NaN                 NaN      01  \n",
      "01.0722               NaN              NaN                 NaN      01  \n",
      "...                   ...              ...                 ...     ...  \n",
      "16.1202               NaN              NaN                 NaN      16  \n",
      "16.1203               NaN              NaN                 NaN      16  \n",
      "16.1204               NaN              NaN                 NaN      16  \n",
      "16.1205               NaN              NaN                 NaN      16  \n",
      "16.1206               NaN              NaN                 NaN      16  \n",
      "\n",
      "[852 rows x 64 columns]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BlockManager' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/qiime2/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/qiime2/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    741\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m         \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selected_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/qiime2/lib/python3.6/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-3db7ad5c426b>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mspecies_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Player\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspecies_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mspecies_table_pct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecies_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Player\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpct_change_manual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mspecies_table_pct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Sample_ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-450b2949bb2f>\u001b[0m in \u001b[0;36mpct_change_manual\u001b[0;34m(df, colnames)\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TWO FECAL SAMPLES FOLLOWING HEAD IMPACT?\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                 \u001b[0;31m##Two days prior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/qiime2/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2199\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2200\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/qiime2/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   2842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/qiime2/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   3270\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3271\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3272\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/qiime2/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_box_item_values\u001b[0;34m(self, key, values)\u001b[0m\n\u001b[1;32m   3461\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3462\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3463\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BlockManager' object has no attribute 'T'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-3db7ad5c426b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mspecies_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaster_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecies_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mspecies_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Player\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspecies_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mspecies_table_pct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecies_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Player\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpct_change_manual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mspecies_table_pct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Sample_ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mspecies_table_pct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecies_table_pct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspecies\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/qiime2/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0m_group_selection_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/qiime2/lib/python3.6/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m         \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selected_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         return self._wrap_applied_output(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/qiime2/lib/python3.6/site-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;31m# group might be modified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-3db7ad5c426b>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mspecies_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaster_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecies_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mspecies_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Player\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspecies_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mspecies_table_pct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecies_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Player\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpct_change_manual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mspecies_table_pct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Sample_ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mspecies_table_pct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecies_table_pct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspecies\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-450b2949bb2f>\u001b[0m in \u001b[0;36mpct_change_manual\u001b[0;34m(df, colnames)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TWO FECAL SAMPLES FOLLOWING HEAD IMPACT?\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                 \u001b[0;31m##Two days prior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/qiime2/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2199\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2200\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/qiime2/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   2841\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_box_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/qiime2/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   3269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3270\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3271\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3272\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3273\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_as_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/qiime2/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_box_item_values\u001b[0;34m(self, key, values)\u001b[0m\n\u001b[1;32m   3460\u001b[0m         \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3461\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3462\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3463\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3464\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_col_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BlockManager' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "order_table = pd.read_csv(\"order-table.tsv\", sep = \"\\t\", skiprows=[0], index_col = 0)\n",
    "order_table = order_table.T\n",
    "order_table.index.names = [\"Sample_ID\"]\n",
    "order_table.drop(order_table.tail(5).index,inplace=True) # drop last n rows\n",
    "for i in range(order_table.shape[1]):\n",
    "    column_name = order_table.columns[i]\n",
    "    if \"o__\" in column_name:\n",
    "        order_table.rename(columns = {column_name:column_name.split(\"o__\", 1)[1]}, inplace= True)\n",
    "order_table.to_csv(\"../order_processed.tsv\", sep =\"\\t\")\n",
    "\n",
    "orders = order_table.columns\n",
    "\n",
    "order_table = master_data.join(order_table)\n",
    "order_table[\"Player\"] = [i.split(\".\")[0] for i in order_table.index.tolist()]\n",
    "\n",
    "print(order_table)\n",
    "order_table_pct = order_table.groupby(\"Player\").apply(lambda df: pct_change_manual(df, orders))\n",
    "order_table_pct.index.names = [\"Sample_ID\"]\n",
    "order_table_pct = order_table_pct[orders]\n",
    "#order_table_pct.reset_index(inplace =True)\n",
    "#order_table_pct.index = order_table.index\n",
    "#order_table_pct.fillna(0, inplace=True)\n",
    "order_table.to_csv(\"../order_processed.tsv\", sep =\"\\t\")\n",
    "order_table_pct.to_csv(\"../order_pct_processed.tsv\", sep = \"\\t\")\n",
    "\n",
    "species_table = pd.read_csv(\"species-table.tsv\", sep = \"\\t\", skiprows=[0], index_col = 0)\n",
    "species_table = species_table.T\n",
    "species_table.index.names = [\"Sample_ID\"]\n",
    "species_table.drop(species_table.tail(5).index,inplace=True) # drop last n rows\n",
    "for i in range(species_table.shape[1]):\n",
    "    column_name = species_table.columns[i]\n",
    "    if \"s__\" in column_name:\n",
    "        species_table.rename(columns = {column_name:column_name.split(\"s__\", 1)[1]}, inplace= True)\n",
    "\n",
    "species = species_table.columns\n",
    "\n",
    "species_table = master_data.join(species_table)\n",
    "species_table[\"Player\"] = [i.split(\".\")[0] for i in species_table.index.tolist()]\n",
    "species_table_pct = species_table.groupby(\"Player\").apply(lambda df: pct_change_manual(df, species))\n",
    "species_table_pct.index.names = [\"Sample_ID\"]\n",
    "species_table_pct = species_table_pct[species]\n",
    "\n",
    "species_table.to_csv(\"../species_processed.tsv\", sep =\"\\t\")\n",
    "species_table_pct.to_csv(\"../species_pct_processed.tsv\", sep =\"\\t\")\n",
    "\n",
    "genus_table = pd.read_csv(\"genus-table.tsv\", sep = \"\\t\", skiprows=[0], index_col = 0)\n",
    "genus_table = genus_table.T\n",
    "genus_table.index.names = [\"Sample_ID\"]\n",
    "genus_table.drop(genus_table.tail(5).index,inplace=True) # drop last n rows\n",
    "for i in range(genus_table.shape[1]):\n",
    "    column_name = genus_table.columns[i]\n",
    "    if \"g__\" in column_name:\n",
    "        genus_table.rename(columns = {column_name:column_name.split(\"g__\", 1)[1]}, inplace= True)\n",
    "genus_table.to_csv(\"../genus_processed.tsv\", sep =\"\\t\")\n",
    "\n",
    "genus = genus_table.columns\n",
    "\n",
    "genus_table[\"Player\"] = [i.split(\".\")[0] for i in genus_table.index.tolist()]\n",
    "genus_table = master_data.join(genus_table)\n",
    "genus_table_pct = genus_table.groupby(\"Player\").apply(lambda df: pct_change_manual(df, genus))\n",
    "genus_table_pct.index.names = [\"Sample_ID\"]\n",
    "genus_table_pct = genus_table_pct[genus]\n",
    "genus_table.to_csv(\"../genus_processed.tsv\", sep =\"\\t\")\n",
    "genus_table.to_csv(\"../genus_pct_processed.tsv\", sep =\"\\t\")\n",
    "\n",
    "family_table = pd.read_csv(\"family-table.tsv\", sep = \"\\t\", skiprows=[0], index_col = 0)\n",
    "family_table = family_table.T\n",
    "family_table.index.names = [\"Sample_ID\"]\n",
    "family_table.drop(family_table.tail(5).index,inplace=True) # drop last n rows\n",
    "for i in range(family_table.shape[1]):\n",
    "    column_name = family_table.columns[i]\n",
    "    if \"f__\" in column_name:\n",
    "        family_table.rename(columns = {column_name:column_name.split(\"f__\", 1)[1]}, inplace= True)\n",
    "\n",
    "family = family_table.columns\n",
    "\n",
    "\n",
    "family_table[\"Player\"] = [i.split(\".\")[0] for i in family_table.index.tolist()]\n",
    "family_table = master_data.join(family_table)\n",
    "family_table_pct = family_table.groupby(\"Player\").apply(lambda df: pct_change_manual(df, family))\n",
    "family_table_pct.index.names = [\"Sample_ID\"]\n",
    "family_table_pct = family_table_pct[family]\n",
    "family_table_pct.to_csv(\"../family_pct_processed.tsv\", sep =\"\\t\")\n",
    "family_table.to_csv(\"../family_processed.tsv\", sep =\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing taxa data for R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_table = pd.read_csv(\"order-table.tsv\", sep = \"\\t\", skiprows=[0], index_col = 0)\n",
    "order_table = order_table.T\n",
    "order_table.index.names = [\"Sample_ID\"]\n",
    "order_table.drop(order_table.tail(5).index,inplace=True) # drop last n rows\n",
    "orders = order_table.columns\n",
    "for i in range(order_table.shape[1]):\n",
    "    column_name = order_table.columns[i]\n",
    "    if \"o__\" in column_name:\n",
    "        order_table.rename(columns = {column_name:column_name.split(\"o__\", 1)[1]}, inplace= True)\n",
    "\n",
    "order_table.loc[\"09.0808\"] = (order_table.loc[\"09.0808.1\"] + order_table.loc[\"09.0808.2\"]) /2\n",
    "order_table.to_csv(\"../order_processed.tsv\", sep =\"\\t\")\n",
    "\n",
    "\n",
    "\n",
    "genus_table = pd.read_csv(\"genus-table.tsv\", sep = \"\\t\", skiprows=[0], index_col = 0)\n",
    "genus_table = genus_table.T\n",
    "genus_table.index.names = [\"Sample_ID\"]\n",
    "genus_table.drop(genus_table.tail(5).index,inplace=True) # drop last n rows\n",
    "for i in range(genus_table.shape[1]):\n",
    "    column_name = genus_table.columns[i]\n",
    "    if \"g__\" in column_name:\n",
    "        genus_table.rename(columns = {column_name:column_name.split(\"g__\", 1)[1]}, inplace= True)\n",
    "\n",
    "genus_table.loc[\"09.0808\"] = (genus_table.loc[\"09.0808.1\"] + genus_table.loc[\"09.0808.2\"]) /2\n",
    "\n",
    "genus_table.to_csv(\"../genus_processed.tsv\", sep =\"\\t\")\n",
    "\n",
    "\n",
    "\n",
    "family_table = pd.read_csv(\"family-table.tsv\", sep = \"\\t\", skiprows=[0], index_col = 0)\n",
    "family_table = family_table.T\n",
    "family_table.index.names = [\"Sample_ID\"]\n",
    "family_table.drop(family_table.tail(5).index,inplace=True) # drop last n rows\n",
    "for i in range(family_table.shape[1]):\n",
    "    column_name = family_table.columns[i]\n",
    "    if \"f__\" in column_name:\n",
    "        family_table.rename(columns = {column_name:column_name.split(\"f__\", 1)[1]}, inplace= True)\n",
    "\n",
    "\n",
    "family_table.loc[\"09.0808\"] = (family_table.loc[\"09.0808.1\"] + family_table.loc[\"09.0808.2\"]) /2\n",
    "\n",
    "family_table.to_csv(\"../family_processed.tsv\", sep =\"\\t\")\n",
    "\n",
    "species_table = pd.read_csv(\"species-table.tsv\", sep = \"\\t\", skiprows=[0], index_col = 0)\n",
    "species_table = species_table.T\n",
    "species_table.index.names = [\"Sample_ID\"]\n",
    "species_table.drop(species_table.tail(5).index,inplace=True) # drop last n rows\n",
    "for i in range(species_table.shape[1]):\n",
    "    column_name = species_table.columns[i]\n",
    "    if \"s__\" in column_name:\n",
    "        species_table.rename(columns = {column_name:column_name.split(\"s__\", 1)[1]}, inplace= True)\n",
    "\n",
    "\n",
    "species_table.loc[\"09.0808\"] = (species_table.loc[\"09.0808.1\"] + species_table.loc[\"09.0808.2\"]) /2\n",
    "\n",
    "species_table.to_csv(\"../species_processed.tsv\", sep =\"\\t\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acute Changes after Head Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_master_no_9 = pd.read_csv(\"../order_master_without_9.tsv\", sep =\"\\t\", dtype={\"Sample_ID\":\"str\"})\n",
    "order_master_no_9 = order_master_no_9.set_index(order_master_no_9.columns[0])\n",
    "\n",
    "genus_master_no_9 = pd.read_csv(\"../genus_master_without_9.tsv\", sep =\"\\t\", dtype={\"Sample_ID\":\"str\"})\n",
    "genus_master_no_9 = genus_master_no_9.set_index(genus_master_no_9.columns[0])\n",
    "\n",
    "family_master_no_9 = pd.read_csv(\"../family_master_without_9.tsv\", sep =\"\\t\", dtype={\"Sample_ID\":\"str\"})\n",
    "family_master_no_9 = family_master_no_9.set_index(family_master_no_9.columns[0])\n",
    "\n",
    "species_master_no_9 = pd.read_csv(\"../species_master_without_9.tsv\", sep =\"\\t\", dtype={\"Sample_ID\":\"str\"})\n",
    "species_master_no_9 = species_master_no_9.set_index(species_master_no_9.columns[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID Player           Time     Value\n",
      "0    0     01  Day of Impact  0.506439\n",
      "1    1     01  Day of Impact  0.425335\n",
      "2    2     01  Day of Impact  0.414519\n",
      "3    3     01  Day of Impact  0.360398\n",
      "4    4     01  Day of Impact  0.423141\n",
      "..  ..    ...            ...       ...\n",
      "75  15     05   3 days after  0.310463\n",
      "76  16     05   3 days after  0.329602\n",
      "77  17     08   3 days after  0.337198\n",
      "78  18     09   3 days after  0.417473\n",
      "79  19     09   3 days after  0.341669\n",
      "\n",
      "[80 rows x 4 columns]\n",
      "    ID Player           Time       Value\n",
      "0    0     01  Day of Impact  165.138992\n",
      "1    1     01  Day of Impact  163.026166\n",
      "2    2     01  Day of Impact  142.122881\n",
      "3    3     01  Day of Impact  160.450072\n",
      "4    4     01  Day of Impact  169.179939\n",
      "..  ..    ...            ...         ...\n",
      "75  15     05   3 days after  202.266162\n",
      "76  16     05   3 days after  196.585848\n",
      "77  17     08   3 days after  194.369543\n",
      "78  18     09   3 days after  167.357732\n",
      "79  19     09   3 days after  186.021487\n",
      "\n",
      "[80 rows x 4 columns]\n",
      "    ID Player           Time     Value\n",
      "0    0     01  Day of Impact  0.859496\n",
      "1    1     01  Day of Impact  0.914638\n",
      "2    2     01  Day of Impact  0.911988\n",
      "3    3     01  Day of Impact  0.957603\n",
      "4    4     01  Day of Impact  0.939375\n",
      "..  ..    ...            ...       ...\n",
      "75  15     05   3 days after  0.973583\n",
      "76  16     05   3 days after  0.971511\n",
      "77  17     08   3 days after  0.964123\n",
      "78  18     09   3 days after  0.966885\n",
      "79  19     09   3 days after  0.942771\n",
      "\n",
      "[80 rows x 4 columns]\n",
      "    ID Player           Time     Value\n",
      "0    0     01  Day of Impact  0.280011\n",
      "1    1     01  Day of Impact  0.295858\n",
      "2    2     01  Day of Impact  0.277495\n",
      "3    3     01  Day of Impact  0.198073\n",
      "4    4     01  Day of Impact  0.280578\n",
      "..  ..    ...            ...       ...\n",
      "75  15     05   3 days after  0.264818\n",
      "76  16     05   3 days after  0.331570\n",
      "77  17     08   3 days after  0.383880\n",
      "78  18     09   3 days after  0.321980\n",
      "79  19     09   3 days after  0.348719\n",
      "\n",
      "[80 rows x 4 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Data Slicing\n",
    "\n",
    "def acute_changes(df, metric, axis_label):\n",
    "    df_new = df\n",
    "    df_new = df_new[df_new['BC_Dissimilarity'].notna()]\n",
    "\n",
    "    threshold = df_new[\"Impact load sustained 0-24 hours prior\"].quantile(0.75)\n",
    "    df_new[\"Sustained_Impact\"] = np.where(df_new[\"Impact load sustained 0-24 hours prior\"]> threshold, 1,0)\n",
    "    \n",
    "    #threshold = df_new[\"IMPACT.LOAD.2.24.HOURS.BEFORE.FECAL.SAMPLE\"].quantile(0.75)\n",
    "    #df_new[\"Sustained_Impact\"] = np.where(df_new[\"IMPACT.LOAD.2.24.HOURS.BEFORE.FECAL.SAMPLE\"]> threshold, 1,0)\n",
    "\n",
    "\n",
    "\n",
    "    global data_slices\n",
    "    data_slices = pd.DataFrame(columns=[\"Day of Impact\", \"1 day after\", \"2 days after\", \"3 days after\", \"Player\"])\n",
    "\n",
    "    df_new.groupby(\"Player\").apply(lambda df: slice_data(df, metric))\n",
    "\n",
    "\n",
    "    data_slices_long = data_slices\n",
    "    data_slices_long[\"ID\"] = data_slices_long.index.values\n",
    "    data_slices_long = pd.melt(data_slices_long, value_vars=[\"Day of Impact\", \"1 day after\", \"2 days after\", \"3 days after\"],\n",
    "    var_name=\"Time\", id_vars= [\"ID\", \"Player\"], value_name=\"Value\")    \n",
    "    data_slices_long['Player'] = data_slices_long['Player'].astype(str)\n",
    "\n",
    "    print(data_slices_long)\n",
    "    \n",
    "    boxplot = sns.boxplot(x='Time',y='Value',data=data_slices_long, color = \"white\", palette=\"Greys\", width=0.6,showfliers = False)\n",
    "    boxplot = sns.stripplot(x='Time',y='Value',hue='Player',data=data_slices_long, s= 6, jitter =False)\n",
    "    boxplot = sns.lineplot(\n",
    "    data=data_slices_long, x=\"Time\", y=\"Value\", units=\"ID\",sort =False,\n",
    "    color=\".6\", estimator=None,alpha=0.7\n",
    "    )\n",
    "    boxplot.set_xlabel(\"Hours since Head Impact\",fontsize=15)\n",
    "    \n",
    "    boxplot.set_xticklabels([\"0-24\", \"24-48\", \"48-72\", \"72-96\"])\n",
    "\n",
    "    boxplot.set_ylabel(axis_label, fontsize=15)\n",
    "    plt.locator_params(axis='y', nbins=4)\n",
    "    boxplot.xaxis.set_tick_params(labelsize=12)\n",
    "    boxplot.yaxis.set_tick_params(labelsize=12)\n",
    "    \n",
    "    handles, previous_labels = boxplot.get_legend_handles_labels()\n",
    "    \n",
    "    lgd = plt.legend(bbox_to_anchor=(1.2, 0.5), loc='center right', borderaxespad=0, title = \"Player\",\n",
    "    fontsize=\"12\",handles=handles, labels=[\"1\",\"4\", \"5\", \"8\", \"9\"])\n",
    "    lgd.get_title().set_fontsize('15') \n",
    "    boxplot.figure.savefig(\"../Figures/\" + metric + \".png\", dpi = 300, bbox_inches = \"tight\")  \n",
    "    plt.cla() \n",
    "    #print(data_slices)\n",
    "    #print(data_slices)\n",
    "    (stat, p) = stats.friedmanchisquare(data_slices[\"Day of Impact\"],\n",
    "     data_slices[\"1 day after\"], data_slices[\"2 days after\"],\n",
    "     data_slices[\"3 days after\"])\n",
    "    with open(\"../acute_results/\" + metric + \".txt\", \"w\") as f:\n",
    "        print(stat, file= f)\n",
    "        print(p, file = f)\n",
    "        print(scikit_posthocs.posthoc_nemenyi_friedman(data_slices.loc[:,[\"Day of Impact\", \"1 day after\", \"2 days after\", \"3 days after\"]]), file = f)\n",
    "\n",
    "def slice_data(df, metric):\n",
    "\n",
    "    for i in range(1, (df.shape[0]-3)):\n",
    "        \n",
    "        if (df.at[df.index.values[i], \"Sustained_Impact\"] == 1) & (df.at[df.index.values[i-1], \"Sustained_Impact\"] == 0):\n",
    "            #check if days are continuous\n",
    "            range_dates = [*range(int(df.index.values[i-1].split(\".\")[1]), int(df.index.values[i-1].split(\".\")[1])+5 )]\n",
    "            #print(range_dates)\n",
    "            actual_dates = [int(k.split(\".\")[1]) for k in df.index.values[i-1:i+4]]\n",
    "            #print(actual_dates)\n",
    "            if np.array_equal(range_dates, actual_dates):\n",
    "            #if True:\n",
    "                \n",
    "                #check for no impacts in i+1, i+2, i+3, i+4, i+5 days\n",
    "                if max(df.loc[df.index.values[i+1:i+4], 'Sustained_Impact'] ==0):\n",
    "                    \n",
    "                    Player = df.index.values[0].split(\".\")[0]\n",
    "                    #values_to_add = [ df.at[df.index.values[i-1], \"simpson\"],\n",
    "                    #    (df.at[df.index.values[i+1], \"simpson\"] +df.at[df.index.values[i+2], \"simpson\"])/2,\n",
    "                    #    ((df.at[df.index.values[i+3], \"simpson\"] +df.at[df.index.values[i+4], \"simpson\"])/2),\n",
    "                    #    df.at[df.index.values[i+5], \"simpson\"] ]\n",
    "                    values_to_add = [ df.at[df.index.values[i], metric],\n",
    "                        df.at[df.index.values[i+1], metric],df.at[df.index.values[i+2], metric],\n",
    "                        df.at[df.index.values[i+3], metric] , Player]\n",
    "                    data_slices.loc[len(data_slices.index)] = values_to_add\n",
    "\n",
    "#print(alpha_master_data)\n",
    "acute_changes(alpha_master_data, metric=\"BC_Dissimilarity\", axis_label=\"Bray Curtis Dissimilarity\")\n",
    "acute_changes(alpha_master_data, metric=\"faith_pd\", axis_label=\"Faith's Phylogenetic Diversity\")\n",
    "acute_changes(alpha_master_data, metric=\"simpson\", axis_label=\"Simpson's Diversity\")\n",
    "acute_changes(alpha_master_data, metric=\"UniFrac_Distance\", axis_label=\"Unifrac Distance\")\n",
    "\n",
    "\n",
    "# acute_changes(df=order_master_no_9, metric = \"Bacteroidales\", axis_label = \"Bacteroidales\")\n",
    "# acute_changes(df=order_master_no_9, metric = \"Bifidobacteriales\", axis_label = \"Bifidobacteriales\")\n",
    "# acute_changes(df=order_master_no_9, metric = \"Lactobacillales\", axis_label = \"Lactobacillales\")\n",
    "# acute_changes(df=order_master_no_9, metric = \"Burkholderiales\", axis_label =\"Burkholderiales\")\n",
    "# acute_changes(df=order_master_no_9, metric = \"Enterobacterales\", axis_label= \"Enterobacterales\")\n",
    "# acute_changes(df=order_master_no_9, metric = \"Pseudomonadales\", axis_label = \"Pseudomonadales\")\n",
    "# acute_changes(df=order_master_no_9, metric = \"Campylobacterales\", axis_label = \"Campylobacterales\")\n",
    "# acute_changes(df=order_master_no_9, metric =  \"Clostridiales\", axis_label =  \"Clostridiales\")\n",
    "# acute_changes(df=order_master_no_9, metric = \"Verrucomicrobiales\", axis_label =  \"Verrucomicrobiales\")\n",
    "\n",
    "\n",
    "# acute_changes(df=family_master_no_9, metric = \"Micrococcaceae\", axis_label = \"Micrococcaceae\")\n",
    "# acute_changes(df=family_master_no_9, metric = \"Prevotellaceae\", axis_label = \"Prevotellaceae\")\n",
    "# acute_changes(df=family_master_no_9, metric = \"Leuconostocaceae\", axis_label = \"Leuconostocaceae\")\n",
    "# acute_changes(df=family_master_no_9, metric = \"Streptococcaceae\", axis_label = \"Streptococcaceae\")\n",
    "# acute_changes(df=family_master_no_9, metric = \"Peptococcaceae\", axis_label = \"Peptococcaceae\")\n",
    "# acute_changes(df=family_master_no_9, metric = \"Lachnospiraceae\", axis_label= \"Lachnospiraceae\")\n",
    "# acute_changes(df=family_master_no_9, metric = \"Bacteroidaceae\", axis_label = \"Bacteroidaceae\")\n",
    "# acute_changes(df=family_master_no_9, metric = \"Rikenellaceae\", axis_label = \"Rikenellaceae\")\n",
    "# acute_changes(df=family_master_no_9, metric = \"Ruminococcaceae\", axis_label = \"Ruminococcaceae\")\n",
    "# acute_changes(df=family_master_no_9, metric = \"Clostridiaceae\", axis_label = \"Clostridiaceae\")\n",
    "# acute_changes(df=family_master_no_9, metric = \"Bifidobacteriaceae\", axis_label =\"Bifidobacteriaceae\")\n",
    "\n",
    "# acute_changes(df=genus_master_no_9, metric = \"Prevotella\", axis_label = \"Prevotella\")\n",
    "# acute_changes(df=genus_master_no_9, metric = \"Weissella\", axis_label = \"Weissella\")\n",
    "# acute_changes(df=genus_master_no_9, metric = \"Lactococcus\", axis_label = \"Lactococcus\")\n",
    "# acute_changes(df=genus_master_no_9, metric = \"Anaerostipes\", axis_label = \"Anaerostipes\")\n",
    "# acute_changes(df=genus_master_no_9, metric = \"Lactobacillus\", axis_label = \"Lactobacillus\")\n",
    "# acute_changes(df=genus_master_no_9, metric = \"Ruminococcus\", axis_label = \"Ruminococcus\")\n",
    "# acute_changes(df=genus_master_no_9, metric = \"Bifidobacterium\", axis_label = \"Bifidobacterium\")\n",
    "# acute_changes(df=genus_master_no_9, metric = \"Bacteroides\", axis_label = \"Bacteroides\")\n",
    "\n",
    "# acute_changes(df=species_master_no_9, metric = \"Anaerostipes_hadrus\", axis_label = \"Anaerostipes_hadrus\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Mid Post Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Pre       Mid      Post Player\n",
      "3   0.389631  0.501608  0.472666     01\n",
      "4   0.337004  0.360398  0.422894     01\n",
      "5   0.461837  0.354922  0.451513     01\n",
      "6   0.229380  0.426920  0.362484     04\n",
      "7   0.271135  0.320549  0.333628     04\n",
      "8   0.268364  0.348748  0.350524     04\n",
      "9   0.203087  0.329335  0.328755     05\n",
      "10  0.293230  0.404449  0.342203     05\n",
      "11  0.308613  0.323907  0.324132     05\n",
      "12  0.242630  0.279798  0.287616     08\n",
      "13  0.268515  0.323298  0.356320     08\n",
      "14  0.266565  0.312442  0.351198     08\n",
      "15  0.254922  0.352110  0.392769     09\n",
      "16  0.337329  0.322823  0.347073     09\n",
      "17  0.288919  0.481162  0.302766     09\n",
      "18  0.195672  0.279802  0.441742     16\n",
      "19  0.301969  0.346126  0.453547     16\n",
      "20  0.561987  0.252232  0.317027     16\n",
      "    ID Player  Time     Value\n",
      "0    3     01   Pre  0.389631\n",
      "1    4     01   Pre  0.337004\n",
      "2    5     01   Pre  0.461837\n",
      "3    6     04   Pre  0.229380\n",
      "4    7     04   Pre  0.271135\n",
      "5    8     04   Pre  0.268364\n",
      "6    9     05   Pre  0.203087\n",
      "7   10     05   Pre  0.293230\n",
      "8   11     05   Pre  0.308613\n",
      "9   12     08   Pre  0.242630\n",
      "10  13     08   Pre  0.268515\n",
      "11  14     08   Pre  0.266565\n",
      "12  15     09   Pre  0.254922\n",
      "13  16     09   Pre  0.337329\n",
      "14  17     09   Pre  0.288919\n",
      "15  18     16   Pre  0.195672\n",
      "16  19     16   Pre  0.301969\n",
      "17  20     16   Pre  0.561987\n",
      "18   3     01   Mid  0.501608\n",
      "19   4     01   Mid  0.360398\n",
      "20   5     01   Mid  0.354922\n",
      "21   6     04   Mid  0.426920\n",
      "22   7     04   Mid  0.320549\n",
      "23   8     04   Mid  0.348748\n",
      "24   9     05   Mid  0.329335\n",
      "25  10     05   Mid  0.404449\n",
      "26  11     05   Mid  0.323907\n",
      "27  12     08   Mid  0.279798\n",
      "28  13     08   Mid  0.323298\n",
      "29  14     08   Mid  0.312442\n",
      "30  15     09   Mid  0.352110\n",
      "31  16     09   Mid  0.322823\n",
      "32  17     09   Mid  0.481162\n",
      "33  18     16   Mid  0.279802\n",
      "34  19     16   Mid  0.346126\n",
      "35  20     16   Mid  0.252232\n",
      "36   3     01  Post  0.472666\n",
      "37   4     01  Post  0.422894\n",
      "38   5     01  Post  0.451513\n",
      "39   6     04  Post  0.362484\n",
      "40   7     04  Post  0.333628\n",
      "41   8     04  Post  0.350524\n",
      "42   9     05  Post  0.328755\n",
      "43  10     05  Post  0.342203\n",
      "44  11     05  Post  0.324132\n",
      "45  12     08  Post  0.287616\n",
      "46  13     08  Post  0.356320\n",
      "47  14     08  Post  0.351198\n",
      "48  15     09  Post  0.392769\n",
      "49  16     09  Post  0.347073\n",
      "50  17     09  Post  0.302766\n",
      "51  18     16  Post  0.441742\n",
      "52  19     16  Post  0.453547\n",
      "53  20     16  Post  0.317027\n",
      "           Pre         Mid        Post Player\n",
      "3   191.228391  143.884209  163.405290     01\n",
      "4   190.319661  160.450072  162.251101     01\n",
      "5    86.814669  150.750261  161.199333     01\n",
      "6   104.376480  132.956063  138.329403     04\n",
      "7   139.687892  146.528575  130.621483     04\n",
      "8   145.077358  146.085422  138.113469     04\n",
      "9   180.027422  193.777025  185.241736     05\n",
      "10  179.300517  208.943819  205.330085     05\n",
      "11  222.535655  214.101376  195.968422     05\n",
      "12  173.042341  196.230100  162.546681     08\n",
      "13  233.326043  164.924375  150.070884     08\n",
      "14  198.707674  161.445488  197.658388     08\n",
      "15  183.543359  169.837290  157.541033     09\n",
      "16  172.121008  169.234467  159.248156     09\n",
      "17  181.148036   71.116810  177.428682     09\n",
      "18  150.347963  170.991828  141.517778     16\n",
      "19  179.040303  127.074119  110.927349     16\n",
      "20  170.014812  145.629518  138.613747     16\n",
      "    ID Player  Time       Value\n",
      "0    3     01   Pre  191.228391\n",
      "1    4     01   Pre  190.319661\n",
      "2    5     01   Pre   86.814669\n",
      "3    6     04   Pre  104.376480\n",
      "4    7     04   Pre  139.687892\n",
      "5    8     04   Pre  145.077358\n",
      "6    9     05   Pre  180.027422\n",
      "7   10     05   Pre  179.300517\n",
      "8   11     05   Pre  222.535655\n",
      "9   12     08   Pre  173.042341\n",
      "10  13     08   Pre  233.326043\n",
      "11  14     08   Pre  198.707674\n",
      "12  15     09   Pre  183.543359\n",
      "13  16     09   Pre  172.121008\n",
      "14  17     09   Pre  181.148036\n",
      "15  18     16   Pre  150.347963\n",
      "16  19     16   Pre  179.040303\n",
      "17  20     16   Pre  170.014812\n",
      "18   3     01   Mid  143.884209\n",
      "19   4     01   Mid  160.450072\n",
      "20   5     01   Mid  150.750261\n",
      "21   6     04   Mid  132.956063\n",
      "22   7     04   Mid  146.528575\n",
      "23   8     04   Mid  146.085422\n",
      "24   9     05   Mid  193.777025\n",
      "25  10     05   Mid  208.943819\n",
      "26  11     05   Mid  214.101376\n",
      "27  12     08   Mid  196.230100\n",
      "28  13     08   Mid  164.924375\n",
      "29  14     08   Mid  161.445488\n",
      "30  15     09   Mid  169.837290\n",
      "31  16     09   Mid  169.234467\n",
      "32  17     09   Mid   71.116810\n",
      "33  18     16   Mid  170.991828\n",
      "34  19     16   Mid  127.074119\n",
      "35  20     16   Mid  145.629518\n",
      "36   3     01  Post  163.405290\n",
      "37   4     01  Post  162.251101\n",
      "38   5     01  Post  161.199333\n",
      "39   6     04  Post  138.329403\n",
      "40   7     04  Post  130.621483\n",
      "41   8     04  Post  138.113469\n",
      "42   9     05  Post  185.241736\n",
      "43  10     05  Post  205.330085\n",
      "44  11     05  Post  195.968422\n",
      "45  12     08  Post  162.546681\n",
      "46  13     08  Post  150.070884\n",
      "47  14     08  Post  197.658388\n",
      "48  15     09  Post  157.541033\n",
      "49  16     09  Post  159.248156\n",
      "50  17     09  Post  177.428682\n",
      "51  18     16  Post  141.517778\n",
      "52  19     16  Post  110.927349\n",
      "53  20     16  Post  138.613747\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pre Mid Post Analysis\n",
    "def perform_pmp(dataset, colname, axislabel, skip_first = True):\n",
    "\n",
    "    global pre_mid_post\n",
    "    pre_mid_post = pd.DataFrame(columns=[\"Pre\", \"Mid\", \"Post\", \"Player\"])\n",
    "    dataset.loc[:,[colname, \"Player\"] ].groupby(\"Player\").apply(lambda x: find_pre_mid_post(x, colname, skip_first) )\n",
    "\n",
    "    pre_mid_post = pre_mid_post.loc[3:,[\"Pre\", \"Mid\", \"Post\", \"Player\"]]\n",
    "    print(pre_mid_post)\n",
    "    \n",
    "    pre_mid_post_long = pre_mid_post\n",
    "    pre_mid_post_long[\"ID\"] = pre_mid_post_long.index.values\n",
    "    pre_mid_post_long = pd.melt(pre_mid_post_long, value_vars=[\"Pre\",\"Mid\",\"Post\"],\n",
    "    var_name=\"Time\", id_vars= [\"ID\", \"Player\"], value_name=\"Value\")    \n",
    "    print(pre_mid_post_long)\n",
    "    pre_mid_post_long['Player'] = pre_mid_post_long['Player'].astype(str)\n",
    "    boxplot = sns.boxplot(x='Time',y='Value',data=pre_mid_post_long, palette=\"Greys\", width=0.5, showfliers=False)\n",
    "    boxplot = sns.stripplot(x='Time',y='Value',hue='Player',data=pre_mid_post_long, s= 6, jitter =False)\n",
    "    boxplot = sns.lineplot(\n",
    "    data=pre_mid_post_long, x=\"Time\", y=\"Value\", units=\"ID\",sort =False,\n",
    "    color=\".7\", estimator=None,alpha=0.7\n",
    "    )\n",
    "    boxplot.set_xlabel(\"Collection Period\",fontsize=15)\n",
    "    \n",
    "    boxplot.set_xticklabels([\"Early\", \"Middle\", \"Late\"])\n",
    "\n",
    "    boxplot.set_ylabel(axislabel, fontsize=15)\n",
    "    plt.locator_params(axis='y', nbins=4)\n",
    "    boxplot.xaxis.set_tick_params(labelsize=12)\n",
    "    boxplot.yaxis.set_tick_params(labelsize=12)\n",
    "    handles, previous_labels = boxplot.get_legend_handles_labels()\n",
    "\n",
    "    lgd = plt.legend(bbox_to_anchor=(1.2, 0.5), loc='center right', borderaxespad=0, title = \"Player\",\n",
    "    fontsize=\"12\", handles = handles, labels = [\"1\",\"4\",\"5\",\"8\",\"9\",\"16\"])\n",
    "    lgd.get_title().set_fontsize('15') \n",
    "    boxplot.figure.savefig(\"../PreMidPost_Figures/\" + colname + \"_pre_mid_post.png\", dpi =300, bbox_inches = \"tight\")\n",
    "    plt.cla()\n",
    "    #(stat, p) = stats.kruskal(pre_mid_post_BC[\"Pre\"], pre_mid_post_BC[\"Mid\"],pre_mid_post_BC[\"Post\"])\n",
    "    #print(stat)\n",
    "    #print(p)\n",
    "    with open(\"../PreMidPost/\" + colname + \".txt\", \"w\") as f:\n",
    "        (stat, p) = stats.friedmanchisquare( pre_mid_post[\"Pre\"], pre_mid_post[\"Mid\"],pre_mid_post[\"Post\"])\n",
    "        print(stat, file =f)\n",
    "        print(p, file = f)\n",
    "\n",
    "        print(scikit_posthocs.posthoc_nemenyi_friedman(pre_mid_post.loc[:, [\"Pre\", \"Mid\", \"Post\"]]), file =f)\n",
    "\n",
    "\n",
    "def find_pre_mid_post(df, colname, skip_first):\n",
    "    nrows = df.shape[0]\n",
    "    center = math.floor(nrows/2)\n",
    "    #pre_mid_post.loc[len(pre_mid_post)] = [df.loc[df.index.values[1:4], colname].mean(), df.loc[df.index.values[center-1:center+2], colname].mean(),df.loc[df.index.values[nrows-3:nrows], colname].mean()]\n",
    "    #pre_mid_post_BC.loc[len(pre_mid_post_BC)] = [df.at[df.index.values[0], colname], df.at[df.index.values[center-1], colname],df.at[df.index.values[nrows-4], colname]]\n",
    "    Player = df.index.values[0].split(\".\")[0]\n",
    "    if skip_first:\n",
    "        if (df.index.values[0].startswith(\"01.\")| df.index.values[0].startswith(\"04.\")):\n",
    "            pre_mid_post.loc[len(pre_mid_post)] = [df.at[df.index.values[1], colname], df.at[df.index.values[center-1], colname],df.at[df.index.values[nrows-5], colname], Player]\n",
    "            pre_mid_post.loc[len(pre_mid_post)] = [df.at[df.index.values[2], colname], df.at[df.index.values[center], colname],df.at[df.index.values[nrows-4], colname], Player]\n",
    "            pre_mid_post.loc[len(pre_mid_post)] = [df.at[df.index.values[3], colname], df.at[df.index.values[center+1], colname],df.at[df.index.values[nrows-3], colname], Player]\n",
    "        \n",
    "        else:\n",
    "            pre_mid_post.loc[len(pre_mid_post)] = [df.at[df.index.values[1], colname], df.at[df.index.values[center-1], colname],df.at[df.index.values[nrows-3], colname], Player]\n",
    "            pre_mid_post.loc[len(pre_mid_post)] = [df.at[df.index.values[2], colname], df.at[df.index.values[center], colname],df.at[df.index.values[nrows-2], colname], Player]\n",
    "            pre_mid_post.loc[len(pre_mid_post)] = [df.at[df.index.values[3], colname], df.at[df.index.values[center+1], colname],df.at[df.index.values[nrows-1], colname], Player]\n",
    "    else:\n",
    "        if (df.index.values[0].startswith(\"01.\")| df.index.values[0].startswith(\"04.\")):\n",
    "            pre_mid_post.loc[len(pre_mid_post)] = [df.at[df.index.values[0], colname], df.at[df.index.values[center-1], colname],df.at[df.index.values[nrows-5], colname], Player]\n",
    "            pre_mid_post.loc[len(pre_mid_post)] = [df.at[df.index.values[1], colname], df.at[df.index.values[center], colname],df.at[df.index.values[nrows-4], colname], Player]\n",
    "            pre_mid_post.loc[len(pre_mid_post)] = [df.at[df.index.values[2], colname], df.at[df.index.values[center+1], colname],df.at[df.index.values[nrows-3], colname], Player]\n",
    "        \n",
    "        else:\n",
    "            pre_mid_post.loc[len(pre_mid_post)] = [df.at[df.index.values[0], colname], df.at[df.index.values[center-1], colname],df.at[df.index.values[nrows-3], colname], Player]\n",
    "            pre_mid_post.loc[len(pre_mid_post)] = [df.at[df.index.values[1], colname], df.at[df.index.values[center], colname],df.at[df.index.values[nrows-2], colname], Player]\n",
    "            pre_mid_post.loc[len(pre_mid_post)] = [df.at[df.index.values[2], colname], df.at[df.index.values[center+1], colname],df.at[df.index.values[nrows-1], colname], Player]\n",
    "\n",
    "perform_pmp(alpha_master_data, \"BC_Dissimilarity\", \"Bray Curtis Dissimilarity\", True)\n",
    "\n",
    "perform_pmp(alpha_master_data, \"faith_pd\", \"Faith's Phylogenetic Diversity\", True)\n",
    "\n",
    "\n",
    "# acute_changes(df=order_master_no_9, metric = \"Bacteroidales\", axis_label = \"Bacteroidales\")\n",
    "# acute_changes(df=order_master_no_9, metric = \"Bifidobacteriales\", axis_label = \"Bifidobacteriales\")\n",
    "# acute_changes(df=order_master_no_9, metric = \"Lactobacillales\", axis_label = \"Lactobacillales\")\n",
    "##perform_pmp(order_master_no_9, \"Coriobacteriales\", \"Coriobacteriales\", False)\n",
    "\n",
    "##perform_pmp(order_master_no_9, \"Burkholderiales\",  \"Burkholderiales\", False)\n",
    "# acute_changes(df=order_master_no_9, metric = \"Enterobacterales\", axis_label= \"Enterobacterales\")\n",
    "# acute_changes(df=order_master_no_9, metric = \"Pseudomonadales\", axis_label = \"Pseudomonadales\")\n",
    "# acute_changes(df=order_master_no_9, metric = \"Campylobacterales\", axis_label = \"Campylobacterales\")\n",
    "# acute_changes(df=order_master_no_9, metric =  \"Clostridiales\", axis_label =  \"Clostridiales\")\n",
    "# acute_changes(df=order_master_no_9, metric = \"Verrucomicrobiales\", axis_label =  \"Verrucomicrobiales\")\n",
    "\n",
    "\n",
    "# acute_changes(df=family_master_no_9, metric = \"Micrococcaceae\", axis_label = \"Micrococcaceae\")\n",
    "##perform_pmp(family_master_no_9, \"Prevotellaceae\", \"Prevotellaceae\", False)\n",
    "# acute_changes(df=family_master_no_9, metric = \"Leuconostocaceae\", axis_label = \"Leuconostocaceae\")\n",
    "# acute_changes(df=family_master_no_9, metric = \"Streptococcaceae\", axis_label = \"Streptococcaceae\")\n",
    "##perform_pmp(family_master_no_9, \"Peptococcaceae\",  \"Peptococcaceae\", False)\n",
    "##perform_pmp(family_master_no_9, \"Lachnospiraceae\", \"Lachnospiraceae\", False)\n",
    "# acute_changes(df=family_master_no_9, metric = \"Bacteroidaceae\", axis_label = \"Bacteroidaceae\")\n",
    "# acute_changes(df=family_master_no_9, metric = \"Rikenellaceae\", axis_label = \"Rikenellaceae\")\n",
    "##perform_pmp(family_master_no_9,  \"Ruminococcaceae\", \"Ruminococcaceae\")\n",
    "# acute_changes(df=family_master_no_9, metric = \"Clostridiaceae\", axis_label = \"Clostridiaceae\")\n",
    "# acute_changes(df=family_master_no_9, metric = \"Bifidobacteriaceae\", axis_label =\"Bifidobacteriaceae\")\n",
    "\n",
    "##perform_pmp(genus_master_no_9, \"Prevotella\", \"Prevotella\", False)\n",
    "##perform_pmp(genus_master_no_9,  \"Weissella\", \"Weissella\", False)\n",
    "# acute_changes(df=genus_master_no_9, metric = \"Lactococcus\", axis_label = \"Lactococcus\")\n",
    "# acute_changes(df=genus_master_no_9, metric = \"Anaerostipes\", axis_label = \"Anaerostipes\")\n",
    "# acute_changes(df=genus_master_no_9, metric = \"Lactobacillus\", axis_label = \"Lactobacillus\")\n",
    "##perform_pmp(genus_master_no_9,  \"Ruminococcus\",  \"Ruminococcus\", False)\n",
    "# acute_changes(df=genus_master_no_9, metric = \"Bifidobacterium\", axis_label = \"Bifidobacterium\")\n",
    "# acute_changes(df=genus_master_no_9, metric = \"Bacteroides\", axis_label = \"Bacteroides\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phyla Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "phyla_table = pd.read_csv(\"phyla-table.tsv\", sep = \"\\t\", skiprows=[0], index_col = 0)\n",
    "phyla_table = phyla_table.T\n",
    "phyla_table.index.names = [\"Sample_ID\"]\n",
    "#phyla_table.drop(phyla_table.tail(5).index,inplace=True) # drop last n rows\n",
    "for i in range(phyla_table.shape[1]):\n",
    "    column_name = phyla_table.columns[i]\n",
    "    if \"p__\" in column_name:\n",
    "        phyla_table.rename(columns = {column_name:column_name.split(\"p__\", 1)[1]}, inplace= True)\n",
    "\n",
    "phyla_table.loc[\"09.0808\"] = (phyla_table.loc[\"09.0808.1\"] + phyla_table.loc[\"09.0808.2\"]) /2\n",
    "phyla_table.rename(columns={'d__Bacteria;__': 'Unclassified'}, inplace=True)\n",
    "phyla_table.to_csv(\"../phyla_processed_with_control.tsv\", sep =\"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Actinobacteriota', 'Bacteroidota', 'Campilobacterota', 'Desulfobacterota', 'Euryarchaeota', 'Firmicutes', 'Proteobacteria', 'Unclassified', 'Verrucomicrobiota']\n"
     ]
    }
   ],
   "source": [
    "phyla = sorted(phyla_table.columns.values.tolist())\n",
    "print(phyla)\n",
    "phyla_master_no_9 = pd.read_csv(\"../phyla_master_without_9.tsv\", sep =\"\\t\", dtype={\"Sample_ID\":\"str\"})\n",
    "phyla_master_no_9 = phyla_master_no_9.set_index(phyla_master_no_9.columns[0])\n",
    "\n",
    "pre_phyla = pd.DataFrame(columns=np.append(phyla, [\"Player\"]))\n",
    "\n",
    "mid_phyla = pd.DataFrame(columns=np.append(phyla, [\"Player\"]))\n",
    "post_phyla = pd.DataFrame(columns=np.append(phyla, [\"Player\"]))\n",
    "def find_pre_mid_post_phyla(df, list_of_phyla):\n",
    "    global pre_phyla\n",
    "    global mid_phyla\n",
    "    global post_phyla\n",
    "    nrows = df.shape[0]\n",
    "    center = math.floor(nrows/2)\n",
    "    Player = df.index.values[0].split(\".\")[0]\n",
    "    if (df.index.values[0].startswith(\"01.\")| df.index.values[0].startswith(\"04.\")):\n",
    "        pre_phyla = pre_phyla.append(pd.DataFrame(df.loc[df.index.values[1:4],phyla].mean()).T.assign(Player = Player), ignore_index=True)\n",
    "        mid_phyla = mid_phyla.append(pd.DataFrame(df.loc[df.index.values[center-1:center+2],phyla].mean()).T.assign(Player=Player), ignore_index=True)\n",
    "        post_phyla = post_phyla.append(pd.DataFrame(df.loc[df.index.values[nrows-5:nrows-2],phyla].mean()).T.assign(Player = Player), ignore_index=True)\n",
    "    else:\n",
    "        pre_phyla = pre_phyla.append(pd.DataFrame(df.loc[df.index.values[1:4],phyla].mean()).T.assign(Player = Player), ignore_index=True)\n",
    "        mid_phyla = mid_phyla.append(pd.DataFrame(df.loc[df.index.values[center-1:center+2],phyla].mean()).T.assign(Player=Player), ignore_index=True)\n",
    "        post_phyla = post_phyla.append(pd.DataFrame(df.loc[df.index.values[nrows-3:nrows],phyla].mean()).T.assign(Player = Player), ignore_index=True)\n",
    "    \n",
    "    \n",
    "phyla_master_no_9.groupby(\"Player\").apply(lambda x: find_pre_mid_post_phyla(x, phyla) )\n",
    "\n",
    "find_pre_mid_post_phyla(phyla_master_no_9, phyla)\n",
    "\n",
    "pre_phyla.drop(index = pre_phyla.head(1).index,inplace=True) \n",
    "pre_phyla.drop(index = pre_phyla.tail(1).index,inplace=True) \n",
    "mid_phyla.drop(index=mid_phyla.head(1).index,inplace=True)\n",
    "mid_phyla.drop(index = mid_phyla.tail(1).index,inplace=True)\n",
    "post_phyla.drop(index =post_phyla.head(1).index,inplace=True) \n",
    "post_phyla.drop(index =post_phyla.tail(1).index,inplace=True) \n",
    "\n",
    "fig, ax = plt.subplots(3,6)\n",
    "\n",
    "for i,time in enumerate([\"Pre\", \"Mid\", \"Post\"]):\n",
    "    player_title = [\"1\", \"4\", \"5\", \"8\", \"9\", \"16\"]\n",
    "    time_label = [\"Early\", \"Middle\", \"Late\"]\n",
    "    for j, Player in enumerate([\"01\", \"04\", \"05\", \"08\", \"09\", \"16\"]):\n",
    "        \n",
    "        if time == \"Pre\":\n",
    "            fracs = pre_phyla.loc[pre_phyla['Player'] == Player, phyla]\n",
    "        elif time == \"Mid\":\n",
    "            fracs = mid_phyla.loc[mid_phyla['Player'] == Player, phyla]\n",
    "        else:\n",
    "            fracs = post_phyla.loc[post_phyla['Player'] == Player, phyla]\n",
    "            ax[i, j].text(0.5, -0.2, player_title[j], transform=ax[i, j].transAxes,\n",
    "                      horizontalalignment='center', verticalalignment='center',\n",
    "                      fontsize =15)\n",
    "            #ax[i,j].set_title(player_title[j], y=-0.01)\n",
    "        ax[i,j].pie(fracs, labels=None,\n",
    "            autopct=None, shadow=False, startangle=90,\n",
    "            colors=[\"blue\",\"orange\",\"red\", \"cyan\",\"pink\",\"purple\",\"green\",\"grey\",\"brown\"])\n",
    "        ax[i,j].set_aspect('equal')\n",
    "            # label y axis\n",
    "        if ax[i,j].is_first_col():\n",
    "            ax[i,j].set_ylabel(time_label[i], fontsize = 15, rotation = 0,labelpad=18)\n",
    "        \n",
    "\n",
    "fig.text(0.55, -0.01, \"Player\", ha=\"center\", fontsize=15)\n",
    "\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "\n",
    "lgd = fig.legend(phyla, loc = 'center right',bbox_to_anchor=(1.3, 0.5))\n",
    "fig.savefig(\"../PreMidPost_Figures/PreMidPost_phyla.png\", dpi = 300, bbox_inches = \"tight\")\n",
    "plt.close()\n",
    "\n",
    "### Stacked Bar Plot\n",
    "fig, ax = plt.subplots(3,1, sharex = True)\n",
    "\n",
    "for i,time in enumerate([\"Early\", \"Middle\", \"Late\"]):\n",
    "    if time == \"Early\":\n",
    "        fracs = pre_phyla\n",
    "    elif time == \"Middle\":\n",
    "        fracs = mid_phyla\n",
    "    else:\n",
    "        fracs = post_phyla\n",
    "    fracs.plot.bar(x = \"Player\", stacked = True, ax=ax[i], legend = False,\n",
    "     color=[\"blue\",\"orange\",\"red\", \"cyan\",\"pink\",\"purple\",\"green\",\"grey\",\"brown\"])\n",
    "   \n",
    "    ax[i].set_title(time, fontsize = 15, loc='left')\n",
    "    ax[i].yaxis.set_tick_params(labelsize=12)\n",
    "ax[1].set_ylabel('Relative Abundance', fontsize = 15)\n",
    "\n",
    "ax[2].set_xlabel(\"Player\", fontsize =15)\n",
    "ax[2].set_xticklabels([\"1\", \"4\", \"5\", \"8\", \"9\", \"16\"], rotation = 0, fontsize =12)\n",
    "plt.tight_layout(pad=0.4, w_pad=0.6, h_pad=1.0)\n",
    "plt.subplots_adjust(wspace=0, hspace=0.5)\n",
    "\n",
    "lgd = fig.legend(phyla, loc = 'center right',bbox_to_anchor=(1.3, 0.5), borderaxespad = 0., title= 'Phyla')\n",
    "lgd.get_title().set_fontsize('15') \n",
    "fig.savefig(\"../PreMidPost_Figures/PreMidPost_phyla_bar.png\", dpi = 300, bbox_inches = \"tight\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "        # Adding metadata for PDF file\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Actinobacteriota  Bacteroidota  Campilobacterota  \\\n",
      "Sample_ID                                                         \n",
      "Internal.Mock          0.009188      0.000000               0.0   \n",
      "NEG.CONT.E             0.034169      0.276885               0.0   \n",
      "NEG.CONT.NE            0.250857      0.072000               0.0   \n",
      "POS.CONT.1             0.000000      0.000000               0.0   \n",
      "POS.CONT.2             0.000000      0.000000               0.0   \n",
      "\n",
      "               Desulfobacterota  Euryarchaeota  Firmicutes  Proteobacteria  \\\n",
      "Sample_ID                                                                    \n",
      "Internal.Mock          0.000000            0.0    0.288864        0.701948   \n",
      "NEG.CONT.E             0.000971            0.0    0.651510        0.036465   \n",
      "NEG.CONT.NE            0.000000            0.0    0.431714        0.245429   \n",
      "POS.CONT.1             0.000000            0.0    0.000000        1.000000   \n",
      "POS.CONT.2             0.000000            0.0    0.000000        1.000000   \n",
      "\n",
      "               Unclassified  Verrucomicrobiota         Player  \n",
      "Sample_ID                                                      \n",
      "Internal.Mock           0.0                0.0  Internal.Mock  \n",
      "NEG.CONT.E              0.0                0.0     NEG.CONT.E  \n",
      "NEG.CONT.NE             0.0                0.0    NEG.CONT.NE  \n",
      "POS.CONT.1              0.0                0.0     POS.CONT.1  \n",
      "POS.CONT.2              0.0                0.0     POS.CONT.2  \n"
     ]
    }
   ],
   "source": [
    "phyla_master_no_9_control = pd.read_csv(\"../phyla_master_without_9_with_control.tsv\", sep =\"\\t\", dtype={\"Sample_ID\":\"str\"})\n",
    "phyla_master_no_9_control = phyla_master_no_9_control.set_index(phyla_master_no_9_control.columns[0])\n",
    "\n",
    "phyla_master_no_9_control_players = phyla_master_no_9_control.drop(\n",
    "    phyla_master_no_9_control.tail(5).index) # drop last n rows\n",
    "\n",
    "phyla_master_no_9_control_players[\"Player\"] = [i.split(\".\")[0] for i in phyla_master_no_9_control_players.index.tolist()]\n",
    "\n",
    "\n",
    "first_data_points = phyla_master_no_9_control_players.groupby(\"Player\").first().reset_index().loc[:,np.append(phyla, [\"Player\"])]\n",
    "controls = phyla_master_no_9_control.tail(5)\n",
    "controls.sort_index(axis=1, inplace=True)\n",
    "controls[\"Player\"] = controls.index.values\n",
    "print(controls)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, sharey = True)\n",
    "\n",
    "\n",
    "first_data_points.plot.bar(x = \"Player\", stacked = True, ax=ax[0], legend = False,\n",
    "    color=[\"blue\",\"orange\",\"red\", \"brown\",\"pink\",\"purple\",\"green\",\"grey\",\"cyan\"])\n",
    "controls.plot.bar(x = \"Player\", stacked = True, ax=ax[1], legend = False,\n",
    "    color=[\"blue\",\"orange\",\"red\", \"brown\",\"pink\",\"purple\",\"green\",\"grey\",\"cyan\"])\n",
    "ax[0].set_ylabel('Relative Abundance', fontsize = 15)\n",
    "ax[0].set_xlabel(\"Player\", fontsize =15)\n",
    "ax[1].set_xlabel(\"Control Samples\", fontsize =15)\n",
    "ax[0].set_xticklabels([\"1\", \"4\", \"5\", \"8\", \"9\", \"16\"], rotation = 0, fontsize =12)\n",
    "ax[1].set_xticklabels([\"Mock\", \"N1\", \"N2\", \"P1\", \"P2\"], rotation = 0, fontsize =12)\n",
    "ax[0].yaxis.set_tick_params(labelsize=12)\n",
    "plt.tight_layout(pad=0.4, w_pad=0.6, h_pad=1.0)\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0)\n",
    "plt.locator_params(axis='y', nbins=3)\n",
    "lgd = fig.legend(phyla, loc = 'center right',bbox_to_anchor=(1.3, 0.5), borderaxespad = 0., title= 'Phyla')\n",
    "lgd.get_title().set_fontsize('15') \n",
    "fig.savefig(\"../PreMidPost_Figures/Control_Bar.png\", dpi = 300, bbox_inches = \"tight\")\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-107-f0a680e5bf46>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-107-f0a680e5bf46>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    pre_mid_post_long = pre_mid_post\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "phyla = phyla_table.columns\n",
    "phyla_master_no_9 = pd.read_csv(\"../phyla_master_without_9.tsv\", sep =\"\\t\", dtype={\"Sample_ID\":\"str\"})\n",
    "phyla_master_no_9 = phyla_master_no_9.set_index(phyla_master_no_9.columns[0])\n",
    "\n",
    "pre_mid_post_phyla = pd.DataFrame(columns=[\"Pre\", \"Mid\", \"Post\", \"Player\", \"Phyla\"])\n",
    "phyla_master_no_9.loc[:,[phyla, \"Player\"] ].groupby(\"Player\").apply(lambda x: find_pre_mid_post(x, colname, skip_first) )\n",
    "\n",
    "pre_mid_post = pre_mid_post.loc[3:,[\"Pre\", \"Mid\", \"Post\", \"Player\", \"Phyla\"]]\n",
    "    \n",
    "    pre_mid_post_long = pre_mid_post\n",
    "    pre_mid_post_long[\"ID\"] = pre_mid_post_long.index.values\n",
    "    pre_mid_post_long = pd.melt(pre_mid_post_long, value_vars=[\"Pre\",\"Mid\",\"Post\"],\n",
    "    var_name=\"Time\", id_vars= [\"ID\", \"Player\"], value_name=\"Value\")    \n",
    "    print(pre_mid_post_long)\n",
    "    pre_mid_post_long['Player'] = pre_mid_post_long['Player'].astype(str)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def find_pre_mid_post(df, colname, skip_first):\n",
    "    nrows = df.shape[0]\n",
    "    center = math.floor(nrows/2)\n",
    "    #pre_mid_post.loc[len(pre_mid_post)] = [df.loc[df.index.values[1:4], colname].mean(), df.loc[df.index.values[center-1:center+2], colname].mean(),df.loc[df.index.values[nrows-3:nrows], colname].mean()]\n",
    "    #pre_mid_post_BC.loc[len(pre_mid_post_BC)] = [df.at[df.index.values[0], colname], df.at[df.index.values[center-1], colname],df.at[df.index.values[nrows-4], colname]]\n",
    "    Player = df.index.values[0].split(\".\")[0]\n",
    "    \n",
    "    \n",
    "    if (df.index.values[0].startswith(\"01.\")| df.index.values[0].startswith(\"04.\")):\n",
    "        pre_mid_post.loc[len(pre_mid_post)] = [df.at[df.index.values[0], colname], df.at[df.index.values[center-1], colname],df.at[df.index.values[nrows-5], colname], Player]\n",
    "        pre_mid_post.loc[len(pre_mid_post)] = [df.at[df.index.values[1], colname], df.at[df.index.values[center], colname],df.at[df.index.values[nrows-4], colname], Player]\n",
    "        pre_mid_post.loc[len(pre_mid_post)] = [df.at[df.index.values[2], colname], df.at[df.index.values[center+1], colname],df.at[df.index.values[nrows-3], colname], Player]\n",
    "    \n",
    "    else:\n",
    "        pre_mid_post.loc[len(pre_mid_post)] = [df.at[df.index.values[0], colname], df.at[df.index.values[center-1], colname],df.at[df.index.values[nrows-3], colname], Player]\n",
    "        pre_mid_post.loc[len(pre_mid_post)] = [df.at[df.index.values[1], colname], df.at[df.index.values[center], colname],df.at[df.index.values[nrows-2], colname], Player]\n",
    "        pre_mid_post.loc[len(pre_mid_post)] = [df.at[df.index.values[2], colname], df.at[df.index.values[center+1], colname],df.at[df.index.values[nrows-1], colname], Player]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_mid_post = pd.DataFrame(columns=[\"Pre\", \"Mid\", \"Post\"])\n",
    "\n",
    "def find_pre_mid_post(df, colname):\n",
    "    global pre_mid_post\n",
    "    nrows = df.shape[0]\n",
    "    center = math.floor(nrows/2)\n",
    "    #pre_mid_post.loc[len(pre_mid_post)] = [df.loc[df.index.values[1:4], colname].mean(), df.loc[df.index.values[center-1:center+2], colname].mean(),df.loc[df.index.values[nrows-3:nrows], colname].mean()]\n",
    "    pre_mid_post.loc[len(pre_mid_post)] = [df.at[df.index.values[0], colname], df.at[df.index.values[center-1], colname],df.at[df.index.values[nrows-4], colname]]\n",
    "    pre_mid_post.loc[len(pre_mid_post)] = [df.at[df.index.values[1], colname], df.at[df.index.values[center], colname],df.at[df.index.values[nrows-3], colname]]\n",
    "    pre_mid_post.loc[len(pre_mid_post)] = [df.at[df.index.values[2], colname], df.at[df.index.values[center+1], colname],df.at[df.index.values[nrows-2], colname]]\n",
    "    pre_mid_post.loc[len(pre_mid_post)] = [df.at[df.index.values[3], colname], df.at[df.index.values[center+2], colname],df.at[df.index.values[nrows-1], colname]]\n",
    "\n",
    "#print(alpha_diversity_master_dummy)\n",
    "alpha_diversity_master_dummy.loc[:,[\"faith_pd\", \"Player\"] ].groupby(\"Player\").apply(lambda x: find_pre_mid_post(x, \"faith_pd\") )\n",
    "\n",
    "pre_mid_post = pre_mid_post.loc[4:,[\"Pre\", \"Mid\", \"Post\"]]\n",
    "print(pre_mid_post)\n",
    "\n",
    "pre_mid_post.boxplot().figure.savefig(\"alpha_pre_mid_post.png\", dpi =300, bbox_inches=\"tight\")\n",
    "(stat, p) = stats.f_oneway(pre_mid_post[\"Pre\"], pre_mid_post[\"Mid\"],pre_mid_post[\"Post\"])\n",
    "print(stat)\n",
    "print(p)\n",
    "\n",
    "(stat, p) = stats.friedmanchisquare( pre_mid_post[\"Pre\"], pre_mid_post[\"Mid\"],pre_mid_post[\"Post\"])\n",
    "print(stat)\n",
    "print(p)\n",
    "\n",
    "print(stats.wilcoxon(pre_mid_post[\"Pre\"], pre_mid_post[\"Post\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metabolomics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "../FAPROTAX_1.2.6/collapse_table.py \\\n",
    "    -i species-table/feature-table.biom \\\n",
    "    -o func_table.tsv \\\n",
    "    -g ../FAPROTAX_1.2.6/FAPROTAX.txt -v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_table = pd.read_csv(\"func_table.tsv\", sep = \"\\t\", index_col = 0)\n",
    "func_table = func_table.T\n",
    "func_table.index.names = [\"Sample_ID\"]\n",
    "func_table.drop(func_table.tail(5).index,inplace=True) # drop last n rows\n",
    "func_table.loc[\"09.0808\"] = (func_table.loc[\"09.0808.1\"] + func_table.loc[\"09.0808.2\"]) /2\n",
    "\n",
    "func_table.to_csv(\"../func_processed.tsv\",sep =\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_table = pd.read_csv(\"order-table.tsv\", sep = \"\\t\", skiprows=[0], index_col = 0)\n",
    "order_table = order_table.T\n",
    "\n",
    "#order_table = order_table.rename(index = {'#OTU ID':'Sample_ID'})\n",
    "order_table.index.names = [\"Sample_ID\"]\n",
    "order_table.drop(order_table.tail(5).index,inplace=True) # drop last n rows\n",
    "for i in range(order_table.shape[1]):\n",
    "    column_name = order_table.columns[i]\n",
    "    if \"o__\" in column_name:\n",
    "        order_table.rename(columns = {column_name:column_name.split(\"o__\", 1)[1]}, inplace= True)\n",
    "#print(order_table)\n",
    "\n",
    "order_table_01 = order_table[order_table.index.str.startswith(\"01.\")]\n",
    "first_row = np.array(order_table_01.iloc[0])\n",
    "\n",
    "#print(first_row)\n",
    "upper_quantile = np.quantile(first_row, 0.75)\n",
    "#print(upper_quantile)\n",
    "\n",
    "above_threshold = first_row > upper_quantile\n",
    "above_threshold = np.array(above_threshold)\n",
    "#above_threshold = np.insert(above_threshold,0, True)\n",
    "#print(above_threshold)\n",
    "\n",
    "order_table_01 = order_table_01.loc[:, np.array(above_threshold)]\n",
    "#print(order_table_01)\n",
    "\n",
    "head_impact_data = pd.read_csv(\"../Master Spreadsheet - Master Data Sheet.csv\", dtype={\"Sample_ID\": \"str\"}) \n",
    "\n",
    "head_impact_data = head_impact_data.iloc[:, [0,2]]\n",
    "head_impact_data = head_impact_data.set_index(head_impact_data.columns[0])\n",
    "\n",
    "head_impact_data_01 = head_impact_data[head_impact_data.index.str.startswith(\"01.\")]\n",
    "\n",
    "ax = order_table_01.plot.bar(stacked = True)\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize = 4.2)\n",
    "ax.xaxis.set_tick_params(labelsize = 3.8)\n",
    "ax.figure.savefig(\"Order_Barplot_01.png\",bbox_inches='tight',dpi=300)\n",
    "\n",
    "ax2 = head_impact_data_01.plot.bar()\n",
    "ax2.xaxis.set_tick_params(labelsize=3.1)\n",
    "ax2.yaxis.set_tick_params(labelsize= 4)\n",
    "ax2.get_legend().remove()\n",
    "ax2.figure.savefig(\"HI_01.png\",bbox_inches='tight',dpi=300)\n",
    "\n",
    "\n",
    "merged_data = order_table_01.combine_first(head_impact_data_01)\n",
    "#print(merged_data)\n",
    "merged_data.drop(labels = \"Impact Load (ROUGH)\", axis = 1, inplace=True)\n",
    "ax3 = merged_data.plot(subplots=True, sharex = True, figsize=(24,16))\n",
    "\n",
    "x = merged_data.index\n",
    "print(x)\n",
    "plt.xticks(np.arange(len(x)), x, rotation =90)\n",
    "for ax_ in ax3:\n",
    "    ax_.yaxis.set_tick_params(labelsize = 8)\n",
    "    ax_.legend(loc = \"upper left\")\n",
    "    ax_addon = ax_.twinx()\n",
    "    ax_addon.bar(x, head_impact_data_01['Impact Load (ROUGH)'], color=\"blue\", alpha=0.5)\n",
    "    ax_addon.tick_params(axis='y', colors=\"blue\")\n",
    "\n",
    "    \n",
    "ax3[0].figure.savefig(\"Lineplots_Order_01.png\",bbox_inches='tight',dpi=300)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_table = pd.read_csv(\"order-table.tsv\", sep = \"\\t\", skiprows=[0], index_col = 0)\n",
    "order_table = order_table.T\n",
    "\n",
    "#order_table = order_table.rename(index = {'#OTU ID':'Sample_ID'})\n",
    "order_table.index.names = [\"Sample_ID\"]\n",
    "order_table.drop(order_table.tail(5).index,inplace=True) # drop last n rows\n",
    "for i in range(order_table.shape[1]):\n",
    "    column_name = order_table.columns[i]\n",
    "    if \"o__\" in column_name:\n",
    "        order_table.rename(columns = {column_name:column_name.split(\"o__\", 1)[1]}, inplace= True)\n",
    "\n",
    "#print(order_table)\n",
    "orders = order_table.columns\n",
    "#order_table = order_table.add(1)\n",
    "order_table[\"Player\"] = [i.split(\".\")[0] for i in order_table.index.tolist()]\n",
    "#print(order_table)\n",
    "order_table_pct = order_table.groupby(\"Player\").apply(lambda x: x.add(0.01).pct_change())\n",
    "order_table_pct.reset_index(inplace =True)\n",
    "order_table_pct.index = order_table.index\n",
    "order_table_pct.fillna(0, inplace=True)\n",
    "print(order_table_pct)\n",
    "#order_table = order_table.groupby(\"Player\").apply(lambda x: x.div(x.iloc[0].add(0.01)))\n",
    "#order_table_pct[\"Player\"] = [i.split(\".\")[0] for i in order_table_pct.index.tolist()]\n",
    "#print(order_table_pct)\n",
    "#order_table = order_table.groupby(\"Player\").apply(lambda x: x.shift(-3))\n",
    "#order_table[\"Player\"] = [i.split(\".\")[0] for i in order_table.index.tolist()]\n",
    "\n",
    "master_data = pd.read_csv(\"../Master Spreadsheet - Master Data Sheet.tsv\", dtype={\"Sample_ID\": \"str\"},\n",
    "    sep = \"\\t\") \n",
    "master_data = master_data.set_index(master_data.columns[0])\n",
    "order_master = order_table_pct.combine_first(master_data)\n",
    "\n",
    "order_master = order_master.dropna()\n",
    "\n",
    "order_master.drop(\"Fecal Sample Before or After Practice/Game?\", axis = 1, inplace= True)\n",
    "columns_to_dummify = [\"Orthopedic Injury\", \"Vomitted\",\"Bristol Stool Scale\", \n",
    "    \"Stool Color\", \"Self-Reported Illness\", \"Fiber\", \"Fiber Supplements\", \"Red Meat\", \"Refined Carbohydrates and Sugar\",\n",
    "    \"NSAIDS\",\"Caffeine\", \"Nicotine Products\", \"Cannabis\", \"Pro-biotics\", \"Creatine\",\n",
    "    \"Pre-Workout\", \"Multi-vitamin\", \"Fish Oil\", \"Vitamin D\", \"Collagen\", \"Prescription Medications\",\n",
    "    \"Change of Living or Dining Circumstance\"]\n",
    "dummy_columns = pd.get_dummies(order_master[columns_to_dummify], columns= columns_to_dummify,drop_first=True)\n",
    "order_master.drop(columns_to_dummify, axis = 1, inplace= True)\n",
    "order_master_dummy = pd.concat([order_master,dummy_columns], axis=1)\n",
    "\n",
    "order_master_dummy.columns = order_master_dummy.columns.str.replace(' ', '_')\n",
    "order_master_dummy.columns = order_master_dummy.columns.str.replace('-', '_')\n",
    "order_master_dummy.columns = order_master_dummy.columns.str.replace('/', '_')\n",
    "order_master_dummy.columns = order_master_dummy.columns.str.replace('(', '')\n",
    "order_master_dummy.columns = order_master_dummy.columns.str.replace(')', '')\n",
    "order_master_dummy.columns = order_master_dummy.columns.str.replace('?', '')\n",
    "order_master_dummy.columns = order_master_dummy.columns.str.replace('.0', '')\n",
    "order_master_dummy.to_csv(\"order_master_processed.csv\")\n",
    "\n",
    "# print(order_master_dummy[order_master_dummy[\"Player\"]!=\"16\"])\n",
    "# model = smf.mixedlm(formula = \"Enterobacterales ~  Impact_Load_ROUGH\", data = order_master_dummy, groups = 'Player').fit()\n",
    "# print(model.summary())\n",
    "\n",
    "# model = smf.mixedlm(formula = \"Bifidobacteriales ~  Impact_Load_ROUGH\", data = order_master_dummy[order_master_dummy[\"Player\"]!=\"16\"], groups = 'Player').fit()\n",
    "# print(model.summary())\n",
    "\n",
    "# model = smf.mixedlm(formula = \"Lachnospirales ~  Impact_Load_ROUGH\", data = order_master_dummy, groups = 'Player').fit()\n",
    "# print(model.summary())\n",
    "\n",
    "# model = smf.ols(\"Bacteroidales ~ Impact_Load_ROUGH\", data = order_master_dummy[order_master_dummy.index.str.startswith(\"01.\")]).fit()\n",
    "# print(model.summary())\n",
    "\n",
    "# model = smf.mixedlm(formula = \"Coriobacteriales ~ Impact_Load_ROUGH\", data = order_master_dummy, groups = 'Player').fit()\n",
    "# print(model.summary())\n",
    "#with open(\"Corio_LM.txt\", \"w\") as f:\n",
    "#    print(model.summary(), file = f)\n",
    "model = smf.mixedlm(formula = \"Coriobacteriales ~Orthopedic_Injury_1+ Orthopedic_Injury_2+  Prescription_Medications_1+ Prescription_Medications_2+ Prescription_Medications_3+ Caffeine_1 + Caffeine_2 + Caffeine_3 + Caffeine_4 + Cannabis_1 + Cannabis_2 + Cannabis_3 + Cannabis_4 + Self_Reported_Illness_1 + Self_Reported_Illness_2+ Self_Reported_Illness_3+Impact_Load_ROUGH + Blood_In_Stool + Bristol_Stool_Scale_1 + Bristol_Stool_Scale_2 + Bristol_Stool_Scale_3 + Bristol_Stool_Scale_4 + Bristol_Stool_Scale_6 + Bristol_Stool_Scale_7\", data = order_master_dummy, groups = 'Player').fit()\n",
    "with open(\"Corio_LM.txt\", \"w\") as f:\n",
    "    print(model.summary(), file = f)\n",
    "\n",
    "#model = smf.ols(\"Coriobacteriales ~ Impact_Load_ROUGH\", data = order_master_dummy[order_master_dummy.index.str.startswith(\"01.\")]).fit()\n",
    "#print(model.summary())\n",
    "\n",
    "model = smf.mixedlm(formula = \"Bifidobacteriales ~  Orthopedic_Injury_1+ Orthopedic_Injury_2+  Prescription_Medications_1+ Prescription_Medications_2+ Prescription_Medications_3+ Caffeine_1 + Caffeine_2 + Caffeine_3 + Caffeine_4 + Cannabis_1 + Cannabis_2 + Cannabis_3 + Cannabis_4 + Self_Reported_Illness_1 + Self_Reported_Illness_2+ Self_Reported_Illness_3+Impact_Load_ROUGH + Blood_In_Stool + Bristol_Stool_Scale_1 + Bristol_Stool_Scale_2 + Bristol_Stool_Scale_3 + Bristol_Stool_Scale_4 + Bristol_Stool_Scale_6 + Bristol_Stool_Scale_7\", data = order_master_dummy, groups = 'Player').fit()\n",
    "with open(\"Bifi_LM.txt\", \"w\") as f:\n",
    "    print(model.summary(), file = f)\n",
    "\n",
    "\n",
    "\n",
    "#model = smf.ols(\"Bifidobacteriales ~ Impact_Load_ROUGH\", data = order_master_dummy[order_master_dummy.index.str.startswith(\"01.\")]).fit()\n",
    "#print(model.summary())\n",
    "model = smf.mixedlm(formula = \"Bifidobacteriales ~  Orthopedic_Injury_1+ Orthopedic_Injury_2+  Prescription_Medications_1+ Prescription_Medications_2+ Prescription_Medications_3+ Caffeine_1 + Caffeine_2 + Caffeine_3 + Caffeine_4 + Cannabis_1 + Cannabis_2 + Cannabis_3 + Cannabis_4 + Self_Reported_Illness_1 + Self_Reported_Illness_2+ Self_Reported_Illness_3+Impact_Load_ROUGH + Blood_In_Stool + Bristol_Stool_Scale_1 + Bristol_Stool_Scale_2 + Bristol_Stool_Scale_3 + Bristol_Stool_Scale_4 + Bristol_Stool_Scale_6 + Bristol_Stool_Scale_7\", data = order_master_dummy, groups = 'Player').fit()\n",
    "with open(\"Bifi_LM.txt\", \"w\") as f:\n",
    "    print(model.summary(), file = f)\n",
    "\n",
    "model = smf.mixedlm(formula = \"Lachnospirales ~   Orthopedic_Injury_1+ Orthopedic_Injury_2+  Prescription_Medications_1+ Prescription_Medications_2+ Prescription_Medications_3+ Caffeine_1 + Caffeine_2 + Caffeine_3 + Caffeine_4 + Cannabis_1 + Cannabis_2 + Cannabis_3 + Cannabis_4 + Self_Reported_Illness_1 + Self_Reported_Illness_2+ Self_Reported_Illness_3+Impact_Load_ROUGH + Blood_In_Stool + Bristol_Stool_Scale_1 + Bristol_Stool_Scale_2 + Bristol_Stool_Scale_3 + Bristol_Stool_Scale_4 + Bristol_Stool_Scale_6 + Bristol_Stool_Scale_7\", data = order_master_dummy, groups = 'Player').fit()\n",
    "with open(\"Lach_LM.txt\", \"w\") as f:\n",
    "    print(model.summary(), file = f)\n",
    "\n",
    "model = smf.mixedlm(formula = \"Bacteroidales  ~Orthopedic_Injury_1+ Orthopedic_Injury_2+  Prescription_Medications_1+ Prescription_Medications_2+ Prescription_Medications_3+ Caffeine_1 + Caffeine_2 + Caffeine_3 + Caffeine_4 + Self_Reported_Illness_1 + Self_Reported_Illness_2+ Self_Reported_Illness_3+Impact_Load_ROUGH + Blood_In_Stool + Bristol_Stool_Scale_1 + Bristol_Stool_Scale_2 + Bristol_Stool_Scale_3 + Bristol_Stool_Scale_4 + Bristol_Stool_Scale_6 + Bristol_Stool_Scale_7\", data = order_master_dummy, groups = 'Player').fit()\n",
    "with open(\"Bacteroidales_LM.txt\", \"w\") as f:\n",
    "    print(model.summary(), file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(order_master_dummy.loc[:,orders])\n",
    "corr_mat = order_master_dummy.loc[:,orders].corr()\n",
    "print(corr_mat)\n",
    "lower_tri = np.tril(corr_mat)\n",
    "cmap = sns.diverging_palette(250, 10, center='light',as_cmap=True)\n",
    "vmax = np.abs(corr_mat).max()\n",
    "print(\"VMAX\",vmax)\n",
    "ax = sns.heatmap(corr_mat, \n",
    "        xticklabels=corr_mat.columns,\n",
    "        yticklabels=corr_mat.columns,\n",
    "        mask = lower_tri,vmax=vmax, vmin= -vmax,\n",
    "        cmap = cmap)\n",
    "ax.xaxis.set_tick_params(labelsize = 5)\n",
    "ax.yaxis.set_tick_params(labelsize = 5)\n",
    "ax.get_figure().savefig(\"Corr_Map_Order_all.png\",bbox_inches='tight',dpi=300)\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols(\"Coriobacteriales ~ Impact_Load_ROUGH+Prescription_Medications_1+ Prescription_Medications_2+ Prescription_Medications_3+ Caffeine_1 + Caffeine_2 + Caffeine_3 + Caffeine_4 + Cannabis_1 + Cannabis_2 + Cannabis_3 + Cannabis_4 + Self_Reported_Illness_1 + Self_Reported_Illness_2+ Self_Reported_Illness_3 + Blood_In_Stool + Bristol_Stool_Scale_1 + Bristol_Stool_Scale_2 + Bristol_Stool_Scale_3 + Bristol_Stool_Scale_4 + Bristol_Stool_Scale_6 + Bristol_Stool_Scale_7\", data = order_master_dummy[order_master_dummy.index.str.startswith(\"01.\")]).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genus_table = pd.read_csv(\"genus-table.tsv\", sep = \"\\t\", skiprows=[0], index_col = 0)\n",
    "genus_table = genus_table.T\n",
    "\n",
    "#order_table = order_table.rename(index = {'#OTU ID':'Sample_ID'})\n",
    "genus_table.index.names = [\"Sample_ID\"]\n",
    "genus_table.drop(genus_table.tail(5).index,inplace=True) # drop last n rows\n",
    "for i in range(genus_table.shape[1]):\n",
    "    column_name = genus_table.columns[i]\n",
    "    if \"g__\" in column_name:\n",
    "        genus_table.rename(columns = {column_name:column_name.split(\"g__\", 1)[1]}, inplace= True)\n",
    "\n",
    "\n",
    "genus_table_01 = genus_table[genus_table.index.str.startswith(\"01.\")]\n",
    "print(genus_table_01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alpha Diversity analysis\n",
    "species_table = pd.read_csv(\"species-table.tsv\", sep = \"\\t\", skiprows=[0], index_col = 0)\n",
    "species_table = species_table.T\n",
    "\n",
    "species_table.drop(species_table.tail(5).index,inplace=True) # drop last n rows\n",
    "for i in range(species_table.shape[1]):\n",
    "    column_name = species_table.columns[i]\n",
    "    if \"s__\" in column_name:\n",
    "        species_table.rename(columns = {column_name:column_name.split(\"s__\", 1)[1]}, inplace= True)\n",
    "\n",
    "print(species_table)\n",
    "#####################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_diversity_table = pd.read_csv(\"species-simpson-table/alpha-diversity.tsv\", sep = \"\\t\",\n",
    "    index_col=0)\n",
    "alpha_diversity_table.drop(alpha_diversity_table.tail(5).index,inplace=True) # drop last n rows\n",
    "\n",
    "alpha_diversity_table = alpha_diversity_table.combine_first(head_impact_data)\n",
    "x = alpha_diversity_table.index\n",
    "plt.xticks(np.arange(len(x)), x, rotation =90, fontsize = 3)\n",
    "plt.plot(alpha_diversity_table[\"simpson\"])\n",
    "ax_alpha_2 = plt.twinx()\n",
    "ax_alpha_2.bar(x, alpha_diversity_table[\"Impact Load (ROUGH)\"],color=\"blue\", alpha=0.5)\n",
    "plt.savefig(\"alpha_barplot.png\",bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(alpha_diversity_table[\"Impact Load (ROUGH)\"], alpha_diversity_table[\"simpson\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_diversity_table = pd.read_csv(\"species-simpson-table/alpha-diversity.tsv\", sep = \"\\t\",\n",
    "    index_col=0)\n",
    "alpha_diversity_table.drop(alpha_diversity_table.tail(5).index,inplace=True) # drop last n rows\n",
    "alpha_diversity_table_01 = alpha_diversity_table[alpha_diversity_table.index.str.startswith(\"01.\")]\n",
    "alpha_diversity_table_01 = alpha_diversity_table_01.pct_change().shift(-3)\n",
    "alpha_diversity_table_01 = alpha_diversity_table_01.combine_first(head_impact_data_01)\n",
    "\n",
    "x = alpha_diversity_table_01.index\n",
    "plt.xticks(np.arange(len(x)), x, rotation =90, fontsize=3)\n",
    "plt.plot(alpha_diversity_table_01[\"simpson\"])\n",
    "ax_alpha_2 = plt.twinx()\n",
    "ax_alpha_2.bar(x, alpha_diversity_table_01[\"Impact Load (ROUGH)\"],color=\"blue\", alpha=0.5)\n",
    "plt.savefig(\"alpha_barplot_01.png\",bbox_inches='tight',dpi=300)\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "x, y = alpha_diversity_table_01[\"Impact Load (ROUGH)\"].values, alpha_diversity_table_01[\"simpson\"].values\n",
    "nas = np.logical_or(np.isnan(x), np.isnan(y))\n",
    "corr, p = stats.pearsonr(x[~nas], y[~nas])\n",
    "print(corr,p)\n",
    "sns.regplot(x = \"Impact Load (ROUGH)\",\n",
    "            y = \"simpson\",\n",
    "            data = alpha_diversity_table_01).get_figure().savefig(\"alpha_v_HI.png\", dpi = 300, bbox_inches = \"tight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_diversity_table = pd.read_csv(\"species-faith-table/alpha-diversity.tsv\", sep = \"\\t\",\n",
    "    index_col=0)\n",
    "alpha_diversity_table.drop(alpha_diversity_table.tail(5).index,inplace=True) # drop last n rows\n",
    "alpha_diversity_table_01 = alpha_diversity_table[alpha_diversity_table.index.str.startswith(\"01.\")]\n",
    "alpha_diversity_table_01 = alpha_diversity_table_01.pct_change()\n",
    "alpha_diversity_table_01 = alpha_diversity_table_01.combine_first(head_impact_data_01)\n",
    "\n",
    "x = alpha_diversity_table_01.index\n",
    "plt.xticks(np.arange(len(x)), x, rotation =90, fontsize=3)\n",
    "plt.plot(alpha_diversity_table_01[\"faith_pd\"])\n",
    "ax_alpha_2 = plt.twinx()\n",
    "ax_alpha_2.bar(x, alpha_diversity_table_01[\"Impact Load (ROUGH)\"],color=\"blue\", alpha=0.5)\n",
    "plt.savefig(\"alpha_barplot_01.png\",bbox_inches='tight',dpi=300)\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "x, y = alpha_diversity_table_01[\"Impact Load (ROUGH)\"].values, alpha_diversity_table_01[\"faith_pd\"].values\n",
    "nas = np.logical_or(np.isnan(x), np.isnan(y))\n",
    "corr, p = stats.pearsonr(x[~nas], y[~nas])\n",
    "print(corr,p)\n",
    "sns.regplot(x = \"Impact Load (ROUGH)\",\n",
    "            y = \"faith_pd\",\n",
    "            data = alpha_diversity_table_01).get_figure().savefig(\"alpha_v_HI.png\", dpi = 300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_data = pd.read_csv(\"../Master Spreadsheet - Master Data Sheet.tsv\", dtype={\"Sample_ID\": \"str\"},\n",
    "    sep = \"\\t\") \n",
    "master_data = master_data.set_index(master_data.columns[0])\n",
    "alpha_diversity_table_master = alpha_diversity_table.combine_first(master_data)\n",
    "\n",
    "alpha_diversity_table_master = alpha_diversity_table_master.dropna()\n",
    "\n",
    "alpha_diversity_table_master.drop(\"Fecal Sample Before or After Practice/Game?\", axis = 1, inplace= True)\n",
    "columns_to_dummify = [\"Orthopedic Injury\", \"Vomitted\",\"Bristol Stool Scale\", \n",
    "    \"Stool Color\", \"Self-Reported Illness\", \"Fiber\", \"Fiber Supplements\", \"Red Meat\", \"Refined Carbohydrates and Sugar\",\n",
    "    \"NSAIDS\",\"Caffeine\", \"Nicotine Products\", \"Cannabis\", \"Pro-biotics\", \"Creatine\",\n",
    "    \"Pre-Workout\", \"Multi-vitamin\", \"Fish Oil\", \"Vitamin D\", \"Collagen\", \"Prescription Medications\",\n",
    "    \"Change of Living or Dining Circumstance\"]\n",
    "dummy_columns = pd.get_dummies(alpha_diversity_table_master[columns_to_dummify], columns= columns_to_dummify,drop_first=True)\n",
    "alpha_diversity_table_master.drop(columns_to_dummify, axis = 1, inplace= True)\n",
    "alpha_diversity_master_dummy = pd.concat([alpha_diversity_table_master,dummy_columns], axis=1)\n",
    "\n",
    "alpha_diversity_master_dummy.columns = alpha_diversity_master_dummy.columns.str.replace(' ', '_')\n",
    "alpha_diversity_master_dummy.columns = alpha_diversity_master_dummy.columns.str.replace('-', '_')\n",
    "alpha_diversity_master_dummy.columns = alpha_diversity_master_dummy.columns.str.replace('/', '_')\n",
    "alpha_diversity_master_dummy.columns = alpha_diversity_master_dummy.columns.str.replace('(', '')\n",
    "alpha_diversity_master_dummy.columns = alpha_diversity_master_dummy.columns.str.replace(')', '')\n",
    "alpha_diversity_master_dummy.columns = alpha_diversity_master_dummy.columns.str.replace('?', '')\n",
    "alpha_diversity_master_dummy.columns = alpha_diversity_master_dummy.columns.str.replace('.0', '')\n",
    "alpha_diversity_master_dummy.to_csv(\"processed_data_alpha_diversity.csv\")\n",
    "\n",
    "independent_var = list(alpha_diversity_master_dummy.columns)\n",
    "independent_var.remove(\"faith_pd\")\n",
    "print(independent_var)\n",
    "x = alpha_diversity_master_dummy[independent_var]\n",
    "regressor = LinearRegression().fit(x, alpha_diversity_master_dummy[\"faith_pd\"])\n",
    "print('Intercept: \\n', regressor.intercept_)\n",
    "print('Coefficients: \\n', regressor.coef_)\n",
    "\n",
    "x = sm.add_constant(x) # adding a constant\n",
    " \n",
    "model = sm.OLS(alpha_diversity_master_dummy[\"faith_pd\"], x).fit()\n",
    "predictions = model.predict(x) \n",
    " \n",
    "print_model = model.summary()\n",
    "f = open(\"linear_model_.txt\", \"w\")\n",
    "print(print_model, file = f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_diversity_master_dummy[\"Player\"] = [i.split(\".\")[0] for i in alpha_diversity_master_dummy.index.tolist()]\n",
    "model = smf.mixedlm(formula = \"faith_pd ~  Orthopedic_Injury_1+Orthopedic_Injury_2+Prescription_Medications_1+ Prescription_Medications_2+ Prescription_Medications_3+ Caffeine_1 + Caffeine_2 + Caffeine_3 + Caffeine_4 + Cannabis_1 + Cannabis_2 + Cannabis_3 + Cannabis_4 + Self_Reported_Illness_1 + Self_Reported_Illness_2+ Self_Reported_Illness_3+Impact_Load_ROUGH + Blood_In_Stool + Bristol_Stool_Scale_1 +Bristol_Stool_Scale_2 + Bristol_Stool_Scale_3 +Bristol_Stool_Scale_4 +Bristol_Stool_Scale_6 +Bristol_Stool_Scale_7 + Suspected_Head_Impact_off_the_Field +Suspected_Head_Impact_on_the_Field + Nicotine_Products_1+Nicotine_Products_2+Nicotine_Products_3+Nicotine_Products_4 +  Creatine_1+Creatine_2 +Change_of_Living_or_Dining_Circumstance_1+Change_of_Living_or_Dining_Circumstance_3\", data = alpha_diversity_master_dummy, groups = \"Player\").fit()\n",
    "print_model = model.summary()\n",
    "f = open(\"mixed_model_pd.txt\", \"w\")\n",
    "print(print_model, file = f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_shift_1 = alpha_diversity_master_dummy.loc[:, [\"faith_pd\", \"Player\"] ].groupby(\"Player\").apply(lambda x: x.shift(-1))\n",
    "\n",
    "time_shift_1[\"Impact_Load_ROUGH\"] = alpha_diversity_master_dummy[\"Impact_Load_ROUGH\"]\n",
    "print(time_shift_1)\n",
    "time_shift_1=time_shift_1.dropna(axis = 0)\n",
    "model = smf.mixedlm(formula = \"faith_pd ~  Impact_Load_ROUGH\", data = time_shift_1, groups = \"Player\").fit()\n",
    "print_model = model.summary()\n",
    "f = open(\"time_shift_1_pd.txt\", \"w\")\n",
    "print(print_model, file = f)\n",
    "f.close()\n",
    "\n",
    "time_shift_2 = alpha_diversity_master_dummy.loc[:, [\"faith_pd\", \"Player\"] ].groupby(\"Player\").apply(lambda x: x.shift(-2))\n",
    "\n",
    "time_shift_2[\"Impact_Load_ROUGH\"] = alpha_diversity_master_dummy[\"Impact_Load_ROUGH\"]\n",
    "print(time_shift_2)\n",
    "time_shift_2=time_shift_2.dropna(axis = 0)\n",
    "model = smf.mixedlm(formula = \"faith_pd ~  Impact_Load_ROUGH\", data = time_shift_2, groups = \"Player\").fit()\n",
    "print_model = model.summary()\n",
    "f = open(\"time_shift_2_pd.txt\", \"w\")\n",
    "print(print_model, file = f)\n",
    "f.close()\n",
    "\n",
    "time_shift_3 = alpha_diversity_master_dummy.loc[:, [\"faith_pd\", \"Player\"] ].groupby(\"Player\").apply(lambda x: x.shift(-3))\n",
    "\n",
    "time_shift_3[\"Impact_Load_ROUGH\"] = alpha_diversity_master_dummy[\"Impact_Load_ROUGH\"]\n",
    "print(time_shift_3)\n",
    "time_shift_3=time_shift_3.dropna(axis = 0)\n",
    "model = smf.mixedlm(formula = \"faith_pd ~  Impact_Load_ROUGH\", data = time_shift_3, groups = \"Player\").fit()\n",
    "print_model = model.summary()\n",
    "f = open(\"time_shift_3_pd.txt\", \"w\")\n",
    "print(print_model, file = f)\n",
    "f.close()\n",
    "\n",
    "cumulative_impact = alpha_diversity_master_dummy.loc[:,[\"Impact_Load_ROUGH\",\"Player\"]].groupby(\"Player\").apply(lambda x:x.rolling(3, min_periods=1).sum())\n",
    "cumulative_impact[\"Player\"] = alpha_diversity_master_dummy[\"Player\"]\n",
    "cumulative_impact[\"faith_pd\"] = alpha_diversity_master_dummy[\"faith_pd\"]\n",
    "cumulative_impact=cumulative_impact.dropna(axis = 0)\n",
    "print(cumulative_impact)\n",
    "model = smf.mixedlm(formula = \"faith_pd ~  Impact_Load_ROUGH\", data = cumulative_impact, groups = \"Player\").fit()\n",
    "print_model = model.summary()\n",
    "f = open(\"cumulative_pd.txt\", \"w\")\n",
    "print(print_model, file = f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_diversity_master_dummy[\"Player\"] = [i.split(\".\")[0] for i in alpha_diversity_master_dummy.index.tolist()]\n",
    "\n",
    "alpha_pct_baseline = alpha_diversity_master_dummy.loc[:, [\"faith_pd\", \"Player\"] ].groupby(\"Player\").apply(lambda x: x.div(x.iloc[0]).subtract(1))\n",
    "alpha_diversity_master_dummy_pct = alpha_diversity_master_dummy\n",
    "alpha_diversity_master_dummy_pct[\"faith_pd\"] = alpha_pct_baseline[\"faith_pd\"]\n",
    "model = smf.mixedlm(formula = \"faith_pd ~  Orthopedic_Injury_1+Orthopedic_Injury_2+Prescription_Medications_1+ Prescription_Medications_2+ Prescription_Medications_3+ Caffeine_1 + Caffeine_2 + Caffeine_3 + Caffeine_4 + Cannabis_1 + Cannabis_2 + Cannabis_3 + Cannabis_4 + Self_Reported_Illness_1 + Self_Reported_Illness_2+ Self_Reported_Illness_3+Impact_Load_ROUGH + Blood_In_Stool + Bristol_Stool_Scale_1 +Bristol_Stool_Scale_2 + Bristol_Stool_Scale_3 +Bristol_Stool_Scale_4 +Bristol_Stool_Scale_6 +Bristol_Stool_Scale_7 + Suspected_Head_Impact_off_the_Field +Suspected_Head_Impact_on_the_Field + Nicotine_Products_1+Nicotine_Products_2+Nicotine_Products_3+Nicotine_Products_4 +  Creatine_1+Creatine_2 +Change_of_Living_or_Dining_Circumstance_1+Change_of_Living_or_Dining_Circumstance_3\", data = alpha_diversity_master_dummy_pct, groups = \"Player\").fit()\n",
    "print_model = model.summary()\n",
    "f = open(\"mixed_model_pd_pct.txt\", \"w\")\n",
    "print(print_model, file = f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpha_pct_change = alpha_diversity_master_dummy.loc[:, [\"faith_pd\", \"Player\"] ].groupby(\"Player\").apply(lambda x: x.pct_change())\n",
    "alpha_pct_change.reset_index(inplace=True)\n",
    "alpha_pct_change.index = alpha_diversity_master_dummy.index\n",
    "alpha_pct_change.fillna(0, inplace=True)\n",
    "alpha_diversity_master_dummy_pct_change = alpha_diversity_master_dummy\n",
    "alpha_diversity_master_dummy_pct_change[\"faith_pd\"] = alpha_pct_change[\"faith_pd\"]\n",
    "print(alpha_diversity_master_dummy_pct_change[\"faith_pd\"])\n",
    "model = smf.mixedlm(formula = \"faith_pd ~  Orthopedic_Injury_1+Orthopedic_Injury_2+Prescription_Medications_1+ Prescription_Medications_2+ Prescription_Medications_3+ Caffeine_1 + Caffeine_2 + Caffeine_3 + Caffeine_4 + Cannabis_1 + Cannabis_2 + Cannabis_3 + Cannabis_4 + Self_Reported_Illness_1 + Self_Reported_Illness_2+ Self_Reported_Illness_3+Impact_Load_ROUGH + Blood_In_Stool + Bristol_Stool_Scale_1 +Bristol_Stool_Scale_2 + Bristol_Stool_Scale_3 +Bristol_Stool_Scale_4 +Bristol_Stool_Scale_6 +Bristol_Stool_Scale_7  + Nicotine_Products_1+Nicotine_Products_2+Nicotine_Products_3+Nicotine_Products_4 +  Creatine_1+Creatine_2 +Change_of_Living_or_Dining_Circumstance_1+Change_of_Living_or_Dining_Circumstance_3\", data = alpha_diversity_master_dummy_pct_change, groups = \"Player\").fit()\n",
    "print_model = model.summary()\n",
    "f = open(\"mixed_model_pd_pct_change.txt\", \"w\")\n",
    "print(print_model, file = f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.mixedlm(formula = \"faith_pd ~  Impact_Load_ROUGH\", data = alpha_diversity_master_dummy_pct_change, groups = \"Player\").fit()\n",
    "print_model = model.summary()\n",
    "f = open(\"mixed_model_pd_pct_change.txt\", \"w\")\n",
    "print(print_model, file = f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_analysis_ad = alpha_diversity_master_dummy_pct_change.loc[:, [\"faith_pd\", \"Player\",\"Impact_Load_ROUGH\"] ]\n",
    "time_analysis_ad[\"Date\"] = [datetime.datetime.strptime(i.split(\".\")[1] + \"2022\", \"%m%d%Y\").date() for i in time_analysis_ad.index.tolist()]\n",
    "time_analysis_ad[\"Days\"]=  time_analysis_ad.loc[:, [\"Date\", \"Player\"]].groupby(\"Player\").apply(lambda x: x.subtract(x.iloc[0]))\n",
    "\n",
    "time_analysis_ad[\"Days\"] = [i.days for i in time_analysis_ad[\"Days\"]]\n",
    "time_analysis_ad[\"Player\"] = alpha_diversity_master_dummy_pct_change[\"Player\"]\n",
    "\n",
    "model = smf.mixedlm(formula = \"faith_pd ~  Impact_Load_ROUGH*Days\", data = time_analysis_ad, groups = \"Player\").fit()\n",
    "print_model = model.summary()\n",
    "f = open(\"mixed_model_pd_pct_change_time_analysis.txt\", \"w\")\n",
    "print(print_model, file = f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Slicing\n",
    "alpha_diversity_master_dummy[\"Sustained_Impact\"] = np.where(alpha_diversity_master_dummy[\"Impact_Load_ROUGH\"]>9, 1,0)\n",
    "\n",
    "#1. separate datasets.\n",
    "player_1 =  alpha_diversity_master_dummy[alpha_diversity_master_dummy[\"Player\"]== \"01\"]\n",
    "player_4 =  alpha_diversity_master_dummy[alpha_diversity_master_dummy[\"Player\"]== \"04\"]\n",
    "player_5 =  alpha_diversity_master_dummy[alpha_diversity_master_dummy[\"Player\"]== \"05\"]\n",
    "player_8 =  alpha_diversity_master_dummy[alpha_diversity_master_dummy[\"Player\"]== \"08\"]\n",
    "player_9 =  alpha_diversity_master_dummy[alpha_diversity_master_dummy[\"Player\"]== \"09\"]\n",
    "player_16 = alpha_diversity_master_dummy[alpha_diversity_master_dummy[\"Player\"]== \"16\"]\n",
    "AD_days = pd.DataFrame(columns=[\"AD_0\", \"AD_1\", \"AD_2\", \"AD_3\", \"AD_4\", \"AD_5\"])\n",
    "#AD_days = pd.DataFrame(columns=[\"AD_0\", \"AD_1_2\", \"AD_3_4\",\"AD_5\"])\n",
    "\n",
    "#2. for each dataset, find relevant slices. Moving window of 7 days.\n",
    "def slice_data(df):\n",
    "    for i in range(1, (df.shape[0]-4)):\n",
    "        if (df.at[df.index.values[i], \"Sustained_Impact\"] == 1) & (df.at[df.index.values[i-1], \"Sustained_Impact\"] == 0):\n",
    "            #check if days are continuous\n",
    "            \n",
    "            range_dates = [*range(int(df.index.values[i-1].split(\".\")[1]), int(df.index.values[i-1].split(\".\")[1])+7 )]\n",
    "            #print(range_dates)\n",
    "            actual_dates = [int(k.split(\".\")[1]) for k in df.index.values[i-1:i+6]]\n",
    "            #print(actual_dates)\n",
    "            #if np.array_equal(range_dates, actual_dates):\n",
    "            if True:\n",
    "                \n",
    "                #check for no impacts in i+1, i+2, i+3, i+4, i+5 days\n",
    "                if max(df.loc[df.index.values[i+1:i+6], 'Sustained_Impact'] ==0):\n",
    "                    \n",
    "                    \n",
    "                    #values_to_add = [ df.at[df.index.values[i-1], \"simpson\"],\n",
    "                    #    (df.at[df.index.values[i+1], \"simpson\"] +df.at[df.index.values[i+2], \"simpson\"])/2,\n",
    "                    #    ((df.at[df.index.values[i+3], \"simpson\"] +df.at[df.index.values[i+4], \"simpson\"])/2),\n",
    "                    #    df.at[df.index.values[i+5], \"simpson\"] ]\n",
    "                    values_to_add = [ df.at[df.index.values[i-1], \"faith_pd\"],\n",
    "                        df.at[df.index.values[i+1], \"faith_pd\"],df.at[df.index.values[i+2], \"faith_pd\"],\n",
    "                        df.at[df.index.values[i+3], \"faith_pd\"], df.at[df.index.values[i+4], \"faith_pd\"],\n",
    "                        df.at[df.index.values[i+5], \"faith_pd\"] ]\n",
    "                    AD_days.loc[len(AD_days.index)] = values_to_add\n",
    "\n",
    "slice_data(player_1)\n",
    "slice_data(player_4)\n",
    "slice_data(player_5)\n",
    "slice_data(player_8)\n",
    "#slice_data(player_9)\n",
    "slice_data(player_16)\n",
    "print(AD_days)\n",
    "\n",
    "AD_days_pct = AD_days.divide(AD_days[\"AD_0\"], axis=\"index\") -1\n",
    "print(AD_days_pct)\n",
    "\n",
    "boxplot = AD_days_pct.boxplot()\n",
    "boxplot.set_ylabel(\"Percent Change in Alpha Diversity\")\n",
    "boxplot.figure.savefig(\"AD_pct_2.png\", dpi = 300, bbox_inches = \"tight\")  \n",
    "\n",
    "(stat, p) = stats.kruskal(AD_days_pct[\"AD_1\"], AD_days_pct[\"AD_2\"],AD_days_pct[\"AD_3\"],AD_days_pct[\"AD_4\"],AD_days_pct[\"AD_5\"])\n",
    "print(stat)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_table = pd.read_csv(\"species-table.tsv\", sep = \"\\t\", skiprows=[0], index_col = 0)\n",
    "species_table = species_table.T\n",
    "\n",
    "#order_table = order_table.rename(index = {'#OTU ID':'Sample_ID'})\n",
    "species_table.index.names = [\"Sample_ID\"]\n",
    "species_table.drop(species_table.tail(5).index,inplace=True) # drop last n rows\n",
    "\n",
    "species = species_table.columns\n",
    "\n",
    "def apply_BC(df):\n",
    "    first_row = df.loc[df.index[0], :].values.flatten().tolist()[0:-2]\n",
    "    for index in df.index.values:\n",
    "        row = df.loc[index, :].values.flatten().tolist()[0:-2]\n",
    "        df.at[index, \"BC_Dissimilarity\"] = braycurtis(row, first_row)\n",
    "    return df\n",
    "species_table[\"Player\"] = [i.split(\".\")[0] for i in species_table.index.tolist()]\n",
    "species_table[\"BC_Dissimilarity\"] = np.nan\n",
    "species_table = species_table.groupby(\"Player\").apply(lambda df: apply_BC(df))\n",
    "print(species_table.loc[:,\"BC_Dissimilarity\"])\n",
    "\n",
    "master_data = pd.read_csv(\"../Master Spreadsheet - Master Data Sheet.tsv\", dtype={\"Sample_ID\": \"str\"},\n",
    "    sep = \"\\t\") \n",
    "master_data = master_data.set_index(master_data.columns[0])\n",
    "species_master = species_table.combine_first(master_data)\n",
    "\n",
    "species_master = species_master.dropna()\n",
    "species_master.drop(\"Fecal Sample Before or After Practice/Game?\", axis = 1, inplace= True)\n",
    "columns_to_dummify = [\"Orthopedic Injury\", \"Vomitted\",\"Bristol Stool Scale\", \n",
    "    \"Stool Color\", \"Self-Reported Illness\", \"Fiber\", \"Fiber Supplements\", \"Red Meat\", \"Refined Carbohydrates and Sugar\",\n",
    "    \"NSAIDS\",\"Caffeine\", \"Nicotine Products\", \"Cannabis\", \"Pro-biotics\", \"Creatine\",\n",
    "    \"Pre-Workout\", \"Multi-vitamin\", \"Fish Oil\", \"Vitamin D\", \"Collagen\", \"Prescription Medications\",\n",
    "    \"Change of Living or Dining Circumstance\"]\n",
    "dummy_columns = pd.get_dummies(species_master[columns_to_dummify], columns= columns_to_dummify,drop_first=True)\n",
    "species_master.drop(columns_to_dummify, axis = 1, inplace= True)\n",
    "species_master_dummy = pd.concat([species_master,dummy_columns], axis=1)\n",
    "\n",
    "species_master_dummy.columns = species_master_dummy.columns.str.replace(' ', '_')\n",
    "species_master_dummy.columns = species_master_dummy.columns.str.replace('-', '_')\n",
    "species_master_dummy.columns = species_master_dummy.columns.str.replace('/', '_')\n",
    "species_master_dummy.columns = species_master_dummy.columns.str.replace('(', '')\n",
    "species_master_dummy.columns = species_master_dummy.columns.str.replace(')', '')\n",
    "species_master_dummy.columns = species_master_dummy.columns.str.replace('?', '')\n",
    "species_master_dummy.columns = species_master_dummy.columns.str.replace('.0', '')\n",
    "species_master_dummy.to_csv(\"species_master_processed.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Slicing\n",
    "species_master_dummy[\"Sustained_Impact\"] = np.where(species_master_dummy[\"Impact_Load_ROUGH\"]>9, 1,0)\n",
    "\n",
    "#1. separate datasets.\n",
    "player_1 =  species_master_dummy[species_master_dummy[\"Player\"]== \"01\"]\n",
    "player_4 =  species_master_dummy[species_master_dummy[\"Player\"]== \"04\"]\n",
    "player_5 =  species_master_dummy[species_master_dummy[\"Player\"]== \"05\"]\n",
    "player_8 =  species_master_dummy[species_master_dummy[\"Player\"]== \"08\"]\n",
    "player_9 =  species_master_dummy[species_master_dummy[\"Player\"]== \"09\"]\n",
    "player_16 = species_master_dummy[species_master_dummy[\"Player\"]== \"16\"]\n",
    "BC_days = pd.DataFrame(columns=[\"BC_0\", \"BC_1\", \"BC_2\", \"BC_3\", \"BC_4\", \"BC_5\"])\n",
    "#BC_days = pd.DataFrame(columns=[\"BC_0\", \"BC_1_2\", \"BC_3_4\",\"BC_5\"])\n",
    "\n",
    "#AD_days = pd.DataFrame(columns=[\"AD_0\", \"AD_1_2\", \"AD_3_4\",\"AD_5\"])\n",
    "\n",
    "#2. for each dataset, find relevant slices. Moving window of 7 days.\n",
    "def slice_data(df):\n",
    "    for i in range(1, (df.shape[0]-4)):\n",
    "        if (df.at[df.index.values[i], \"Sustained_Impact\"] == 1) & (df.at[df.index.values[i-1], \"Sustained_Impact\"] == 0):\n",
    "            #check if days are continuous\n",
    "            \n",
    "            range_dates = [*range(int(df.index.values[i-1].split(\".\")[1]), int(df.index.values[i-1].split(\".\")[1])+7 )]\n",
    "            #print(range_dates)\n",
    "            actual_dates = [int(k.split(\".\")[1]) for k in df.index.values[i-1:i+6]]\n",
    "            #print(actual_dates)\n",
    "            #if np.array_equal(range_dates, actual_dates):\n",
    "            if True:\n",
    "                \n",
    "                #check for no impacts in i+1, i+2, i+3, i+4, i+5 days\n",
    "                if max(df.loc[df.index.values[i+1:i+6], 'Sustained_Impact'] ==0):\n",
    "                    \n",
    "                    \n",
    "                    #values_to_add = [ df.at[df.index.values[i-1], \"BC_Dissimilarity\"],\n",
    "                    #    (df.at[df.index.values[i+1], \"BC_Dissimilarity\"] +df.at[df.index.values[i+2], \"BC_Dissimilarity\"])/2,\n",
    "                    #    ((df.at[df.index.values[i+3], \"BC_Dissimilarity\"] +df.at[df.index.values[i+4], \"BC_Dissimilarity\"])/2),\n",
    "                    #    df.at[df.index.values[i+5], \"BC_Dissimilarity\"] ]\n",
    "                    values_to_add = [ df.at[df.index.values[i], \"BC_Dissimilarity\"],\n",
    "                        df.at[df.index.values[i+1], \"BC_Dissimilarity\"],df.at[df.index.values[i+2], \"BC_Dissimilarity\"],\n",
    "                        df.at[df.index.values[i+3], \"BC_Dissimilarity\"], df.at[df.index.values[i+4], \"BC_Dissimilarity\"],\n",
    "                        df.at[df.index.values[i+5], \"BC_Dissimilarity\"] ]\n",
    "                    BC_days.loc[len(BC_days.index)] = values_to_add\n",
    "\n",
    "slice_data(player_1)\n",
    "slice_data(player_4)\n",
    "slice_data(player_5)\n",
    "slice_data(player_8)\n",
    "#slice_data(player_9)\n",
    "slice_data(player_16)\n",
    "\n",
    "#BC_days_pct = BC_days.divide(BC_days[\"BC_0\"], axis=\"index\") -1\n",
    "\n",
    "#print(BC_days_pct)\n",
    "\n",
    "boxplot = BC_days.boxplot()\n",
    "boxplot.set_ylabel(\"Bray-Curtis Disimilarity\")\n",
    "boxplot.figure.savefig(\"BC_2.png\", dpi = 300, bbox_inches = \"tight\")  \n",
    "(stat, p) = stats.kruskal( BC_days[\"BC_0\"], BC_days[\"BC_1\"], BC_days[\"BC_2\"], BC_days[\"BC_3\"], BC_days[\"BC_4\"],BC_days[\"BC_5\"])\n",
    "#(stat, p) = stats.kruskal(BC_days[\"BC_0\"], BC_days[\"BC_1_2\"], BC_days[\"BC_3_4\"], BC_days[\"BC_5\"])\n",
    "\n",
    "print(stat)\n",
    "print(p)\n",
    "(stat, p) = stats.friedmanchisquare( BC_days[\"BC_0\"], BC_days[\"BC_1\"], BC_days[\"BC_2\"], BC_days[\"BC_3\"], BC_days[\"BC_4\"],BC_days[\"BC_5\"])\n",
    "#(stat, p) = stats.friedmanchisquare(BC_days[\"BC_0\"], BC_days[\"BC_1_2\"], BC_days[\"BC_3_4\"], BC_days[\"BC_5\"])\n",
    "\n",
    "print(stat)\n",
    "print(p)\n",
    "\n",
    "print(stats.wilcoxon(BC_days[\"BC_0\"], BC_days[\"BC_1\"]))\n",
    "print(stats.wilcoxon(BC_days[\"BC_0\"], BC_days[\"BC_2\"]))\n",
    "print(stats.wilcoxon(BC_days[\"BC_0\"], BC_days[\"BC_3\"]))\n",
    "print(stats.wilcoxon(BC_days[\"BC_0\"], BC_days[\"BC_4\"]))\n",
    "print(stats.wilcoxon(BC_days[\"BC_0\"], BC_days[\"BC_5\"]))\n",
    "print(stats.wilcoxon(BC_days[\"BC_1\"], BC_days[\"BC_2\"]))\n",
    "print(stats.wilcoxon(BC_days[\"BC_1\"], BC_days[\"BC_3\"]))\n",
    "print(stats.wilcoxon(BC_days[\"BC_1\"], BC_days[\"BC_4\"]))\n",
    "print(stats.wilcoxon(BC_days[\"BC_1\"], BC_days[\"BC_5\"]))\n",
    "print(stats.wilcoxon(BC_days[\"BC_2\"], BC_days[\"BC_3\"]))\n",
    "print(stats.wilcoxon(BC_days[\"BC_2\"], BC_days[\"BC_4\"]))\n",
    "print(stats.wilcoxon(BC_days[\"BC_2\"], BC_days[\"BC_5\"]))\n",
    "print(stats.wilcoxon(BC_days[\"BC_3\"], BC_days[\"BC_4\"]))\n",
    "print(stats.wilcoxon(BC_days[\"BC_3\"], BC_days[\"BC_5\"]))\n",
    "print(stats.wilcoxon(BC_days[\"BC_4\"], BC_days[\"BC_5\"]))\n",
    "\n",
    "\n",
    "print(stats.ttest_rel(BC_days[\"BC_0\"], BC_days[\"BC_1\"]))\n",
    "print(stats.ttest_rel(BC_days[\"BC_0\"], BC_days[\"BC_2\"]))\n",
    "print(stats.ttest_rel(BC_days[\"BC_0\"], BC_days[\"BC_3\"]))\n",
    "print(stats.ttest_rel(BC_days[\"BC_0\"], BC_days[\"BC_4\"]))\n",
    "print(stats.ttest_rel(BC_days[\"BC_0\"], BC_days[\"BC_5\"]))\n",
    "#print(stats.wilcoxon(BC_days[\"BC_1\"], BC_days[\"BC_2\"]))\n",
    "#print(stats.wilcoxon(BC_days[\"BC_1\"], BC_days[\"BC_3\"]))\n",
    "#print(stats.wilcoxon(BC_days[\"BC_1\"], BC_days[\"BC_4\"]))\n",
    "#print(stats.wilcoxon(BC_days[\"BC_1\"], BC_days[\"BC_5\"]))\n",
    "#print(stats.wilcoxon(BC_days[\"BC_2\"], BC_days[\"BC_3\"]))\n",
    "#print(stats.wilcoxon(BC_days[\"BC_2\"], BC_days[\"BC_4\"]))\n",
    "#print(stats.wilcoxon(BC_days[\"BC_2\"], BC_days[\"BC_5\"]))\n",
    "#print(stats.wilcoxon(BC_days[\"BC_3\"], BC_days[\"BC_4\"]))\n",
    "#print(stats.wilcoxon(BC_days[\"BC_3\"], BC_days[\"BC_5\"]))\n",
    "#print(stats.wilcoxon(BC_days[\"BC_4\"], BC_days[\"BC_5\"]))\n",
    "p_vals = [0.12939453125, 0.38037109375,0.04248046875, 0.12939453125, 0.05224609375, 0.5693359375,0.20361328125, 0.85009765625,0.20361328125,0.01611328125,0.20361328125,0.06396484375,0.20361328125,0.5185546875,0.38037109375]\n",
    "(rej, adj_p) = sm.stats.fdrcorrection(p_vals)\n",
    "print(adj_p)\n",
    "\n",
    "BC_days[\"ID\"] = BC_days.index.values\n",
    "BC_days_long = pd.melt(BC_days, value_vars=[\"BC_0\",\"BC_1\", \"BC_2\", \"BC_3\",\"BC_4\",\"BC_5\"],\n",
    "    var_name=\"Days\", id_vars=\"ID\", value_name=\"BC\")\n",
    "print(BC_days_long)\n",
    "\n",
    "aovrm = AnovaRM(BC_days_long, 'BC', 'ID', within=['Days'])\n",
    "res = aovrm.fit()\n",
    "print(res)\n",
    "\n",
    "\n",
    "model = smf.mixedlm(formula = \"BC_Dissimilarity ~  Orthopedic_Injury_1+Orthopedic_Injury_2+Prescription_Medications_1+ Prescription_Medications_2+ Prescription_Medications_3+ Caffeine_1 + Caffeine_2 + Caffeine_3 + Caffeine_4 + Cannabis_1 + Cannabis_2 + Cannabis_3 + Cannabis_4 + Self_Reported_Illness_1 + Self_Reported_Illness_2+ Self_Reported_Illness_3+Impact_Load_ROUGH + Blood_In_Stool + Bristol_Stool_Scale_1 +Bristol_Stool_Scale_2 + Bristol_Stool_Scale_3 +Bristol_Stool_Scale_4 +Bristol_Stool_Scale_6 +Bristol_Stool_Scale_7 + Suspected_Head_Impact_off_the_Field +Suspected_Head_Impact_on_the_Field + Nicotine_Products_1+Nicotine_Products_2+Nicotine_Products_3+Nicotine_Products_4 +  Creatine_1+Creatine_2 +Change_of_Living_or_Dining_Circumstance_1+Change_of_Living_or_Dining_Circumstance_3\", data = species_master_dummy, groups = \"Player\").fit()\n",
    "print_model = model.summary()\n",
    "f = open(\"mixed_model_BC.txt\", \"w\")\n",
    "print(print_model, file = f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_analysis = species_master_dummy.loc[:, [\"BC_Dissimilarity\", \"Player\",\"Impact_Load_ROUGH\"] ]\n",
    "time_analysis[\"Date\"] = [datetime.datetime.strptime(i.split(\".\")[1] + \"2022\", \"%m%d%Y\").date() for i in time_analysis.index.tolist()]\n",
    "time_analysis[\"Days\"]=  time_analysis.loc[:, [\"Date\", \"Player\"]].groupby(\"Player\").apply(lambda x: x.subtract(x.iloc[0]))\n",
    "\n",
    "time_analysis[\"Days\"] = [i.days for i in time_analysis[\"Days\"]]\n",
    "time_analysis[\"Player\"] = species_master_dummy[\"Player\"]\n",
    "print(time_analysis)\n",
    "\n",
    "model = smf.mixedlm(formula = \"BC_Dissimilarity ~  Impact_Load_ROUGH *Days\", data = time_analysis, groups = \"Player\").fit()\n",
    "print_model = model.summary()\n",
    "f = open(\"time_analysis_BC.txt\", \"w\")\n",
    "print(print_model, file = f)\n",
    "f.close()\n",
    "\n",
    "time_analysis.to_csv(\"time_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_shift_1_BC = species_master_dummy.loc[:, [\"BC_Dissimilarity\", \"Player\"] ].groupby(\"Player\").apply(lambda x: x.shift(-1))\n",
    "\n",
    "time_shift_1_BC[\"Impact_Load_ROUGH\"] = species_master_dummy[\"Impact_Load_ROUGH\"]\n",
    "print(time_shift_1_BC)\n",
    "time_shift_1_BC = time_shift_1_BC.dropna(axis = 0)\n",
    "model = smf.mixedlm(formula = \"BC_Dissimilarity ~  Impact_Load_ROUGH\", data = time_shift_1_BC, groups = \"Player\").fit()\n",
    "print_model = model.summary()\n",
    "f = open(\"time_shift_1_BC.txt\", \"w\")\n",
    "print(print_model, file = f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "time_shift_2_BC = species_master_dummy.loc[:, [\"BC_Dissimilarity\", \"Player\"] ].groupby(\"Player\").apply(lambda x: x.shift(-2))\n",
    "\n",
    "time_shift_2_BC[\"Impact_Load_ROUGH\"] = species_master_dummy[\"Impact_Load_ROUGH\"]\n",
    "print(time_shift_2_BC)\n",
    "time_shift_2_BC = time_shift_2_BC.dropna(axis = 0)\n",
    "model = smf.mixedlm(formula = \"BC_Dissimilarity ~  Impact_Load_ROUGH\", data = time_shift_2_BC, groups = \"Player\").fit()\n",
    "print_model = model.summary()\n",
    "f = open(\"time_shift_2_BC.txt\", \"w\")\n",
    "print(print_model, file = f)\n",
    "f.close()\n",
    "\n",
    "time_shift_3_BC = species_master_dummy.loc[:, [\"BC_Dissimilarity\", \"Player\"] ].groupby(\"Player\").apply(lambda x: x.shift(-3))\n",
    "\n",
    "time_shift_3_BC[\"Impact_Load_ROUGH\"] = species_master_dummy[\"Impact_Load_ROUGH\"]\n",
    "print(time_shift_3_BC)\n",
    "time_shift_3_BC=time_shift_3_BC.dropna(axis = 0)\n",
    "model = smf.mixedlm(formula = \"BC_Dissimilarity ~  Impact_Load_ROUGH\", data = time_shift_3_BC, groups = \"Player\").fit()\n",
    "print_model = model.summary()\n",
    "f = open(\"time_shift_3_BC.txt\", \"w\")\n",
    "print(print_model, file = f)\n",
    "f.close()\n",
    "\n",
    "cumulative_impact_BC = species_master_dummy.loc[:,[\"Impact_Load_ROUGH\",\"Player\"]].groupby(\"Player\").apply(lambda x:x.rolling(3, min_periods=1).sum())\n",
    "cumulative_impact_BC[\"Player\"] = species_master_dummy[\"Player\"]\n",
    "cumulative_impact_BC[\"BC_Dissimilarity\"] = species_master_dummy[\"BC_Dissimilarity\"]\n",
    "cumulative_impact_BC = cumulative_impact_BC.dropna(axis = 0)\n",
    "print(cumulative_impact_BC)\n",
    "model = smf.mixedlm(formula = \"BC_Dissimilarity ~  Impact_Load_ROUGH\", data = cumulative_impact_BC, groups = \"Player\").fit()\n",
    "print_model = model.summary()\n",
    "f = open(\"cumulative_BC.txt\", \"w\")\n",
    "print(print_model, file = f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_df = pd.DataFrame(columns=player_1.columns)\n",
    "def slice_data_for_PCoA(df):\n",
    "    global sliced_df\n",
    "    for i in range(1, (df.shape[0]-4)):\n",
    "        if (df.at[df.index.values[i], \"Sustained_Impact\"] == 1) & (df.at[df.index.values[i-1], \"Sustained_Impact\"] == 0):\n",
    "            #check if days are continuous\n",
    "            \n",
    "            range_dates = [*range(int(df.index.values[i-1].split(\".\")[1]), int(df.index.values[i-1].split(\".\")[1])+7 )]\n",
    "            #print(range_dates)\n",
    "            actual_dates = [int(k.split(\".\")[1]) for k in df.index.values[i-1:i+6]]\n",
    "            #print(actual_dates)\n",
    "            #if np.array_equal(range_dates, actual_dates):\n",
    "            if True:\n",
    "                \n",
    "                #check for no impacts in i+1, i+2, i+3, i+4, i+5 days\n",
    "                if max(df.loc[df.index.values[i+1:i+6], 'Sustained_Impact'] ==0):\n",
    "                    \n",
    "                    \n",
    "                    #values_to_add = [ df.at[df.index.values[i-1], \"BC_Dissimilarity\"],\n",
    "                    #    (df.at[df.index.values[i+1], \"BC_Dissimilarity\"] +df.at[df.index.values[i+2], \"BC_Dissimilarity\"])/2,\n",
    "                    #    ((df.at[df.index.values[i+3], \"BC_Dissimilarity\"] +df.at[df.index.values[i+4], \"BC_Dissimilarity\"])/2),\n",
    "                    #    df.at[df.index.values[i+5], \"BC_Dissimilarity\"] ]\n",
    "                    \n",
    "                    sliced_df = sliced_df.append(df.loc[df.index.values[i-1:i+6]])\n",
    "\n",
    "\n",
    "slice_data_for_PCoA(player_1)\n",
    "slice_data_for_PCoA(player_4)\n",
    "slice_data_for_PCoA(player_5)\n",
    "slice_data_for_PCoA(player_8)\n",
    "#slice_data_for_PCoA(player_9)\n",
    "#slice_data_for_PCoA(player_16)\n",
    "\n",
    "species = [i for i in species_master_dummy.columns if \"d__\" in i]\n",
    "sliced_df=sliced_df.loc[:,species]\n",
    "print(sliced_df)\n",
    "print(pdist(sliced_df))\n",
    "matrix = squareform(pdist(sliced_df, metric = \"euclidean\"))\n",
    "\n",
    "\n",
    "print(matrix)\n",
    "pcoa_results = pcoa(matrix)\n",
    "pcoa_df = pcoa_results.samples[['PC1', 'PC2']]\n",
    "print(pcoa_df.shape)\n",
    "a = np.array([\"Day Pre\", \"Day of Impact\", \"1 day after\", \"2 days after\", \"3 days after\", \"4 days after\", \"5 days after\"])\n",
    "pcoa_df[\"Day\"] = np.tile(a,int(sliced_df.shape[0]/7))\n",
    "plt.scatter(pcoa_df.loc[pcoa_df[\"Day\"] == \"Day Pre\", \"PC1\"], \n",
    "    pcoa_df.loc[pcoa_df[\"Day\"] == \"Day Pre\", \"PC2\"],c = 'black')\n",
    "\n",
    "plt.scatter(pcoa_df.loc[pcoa_df[\"Day\"] == \"Day of Impact\", \"PC1\"], \n",
    "    pcoa_df.loc[pcoa_df[\"Day\"] == \"Day of Impact\", \"PC2\"],c = 'blue')\n",
    "\n",
    "plt.scatter(pcoa_df.loc[pcoa_df[\"Day\"] == \"1 day after\", \"PC1\"], \n",
    "    pcoa_df.loc[pcoa_df[\"Day\"] == \"1 day after\", \"PC2\"],c = 'red')\n",
    "\n",
    "\n",
    "plt.scatter(pcoa_df.loc[pcoa_df[\"Day\"] == \"2 days after\", \"PC1\"], \n",
    "    pcoa_df.loc[pcoa_df[\"Day\"] == \"2 days after\", \"PC2\"],c = 'green')\n",
    "\n",
    "plt.scatter(pcoa_df.loc[pcoa_df[\"Day\"] == \"3 days after\", \"PC1\"], \n",
    "    pcoa_df.loc[pcoa_df[\"Day\"] == \"3 days after\", \"PC2\"],c = 'purple')\n",
    "\n",
    "def encircle2(x,y, ax=None, **kw):\n",
    "    if not ax: ax=plt.gca()\n",
    "    p = np.c_[x,y]\n",
    "    mean = np.mean(p, axis=0)\n",
    "    d = p-mean\n",
    "    r = np.max(np.sqrt(d[:,0]**2+d[:,1]**2 ))\n",
    "    circ = plt.Circle(mean, radius=1.05*r,**kw)\n",
    "    ax.add_patch(circ)\n",
    "\n",
    "encircle2(pcoa_df.loc[pcoa_df[\"Day\"] == \"Day Pre\", \"PC1\"], \n",
    "    pcoa_df.loc[pcoa_df[\"Day\"] == \"Day Pre\", \"PC2\"], ec=\"black\", fc=\"none\")\n",
    "encircle2( pcoa_df.loc[pcoa_df[\"Day\"] == \"Day of Impact\", \"PC1\"], \n",
    "    pcoa_df.loc[pcoa_df[\"Day\"] == \"Day of Impact\", \"PC2\"], ec = 'blue',fc=\"none\")\n",
    "encircle2(pcoa_df.loc[pcoa_df[\"Day\"] == \"1 day after\", \"PC1\"], \n",
    "    pcoa_df.loc[pcoa_df[\"Day\"] == \"1 day after\", \"PC2\"],ec = 'red', fc = \"none\")\n",
    "encircle2(pcoa_df.loc[pcoa_df[\"Day\"] == \"2 days after\", \"PC1\"], \n",
    "    pcoa_df.loc[pcoa_df[\"Day\"] == \"2 days after\", \"PC2\"],ec = 'green', fc = \"none\")\n",
    "encircle2(pcoa_df.loc[pcoa_df[\"Day\"] == \"3 days after\", \"PC1\"], \n",
    "    pcoa_df.loc[pcoa_df[\"Day\"] == \"3 days after\", \"PC2\"],ec = 'purple', fc=\"none\")\n",
    "\n",
    "plt.gca().relim()\n",
    "plt.gca().autoscale_view()\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend([\"Day Pre\", \"Day of Impact\", \"1 day after\", \"2 days after\", \"3 days after\"])\n",
    "plt.savefig(\"PcoA_01.png\", dpi = 300, bbox_inches=\"tight\")\n",
    "\n",
    "#dm = DistanceMatrix(matrix, ids = species.values)\n",
    "#print(dm)\n",
    "\n",
    "#slice_data_for_PCoA(player_4)\n",
    "#slice_data_for_PCoA(player_5)\n",
    "#slice_data_for_PCoA(player_8)\n",
    "#slice_data_for_PCoA(player_9)\n",
    "#slice_data_for_PCoA(player_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_df = pd.DataFrame(columns=player_1.columns)\n",
    "\n",
    "slice_data_for_PCoA(player_1)\n",
    "slice_data_for_PCoA(player_4)\n",
    "slice_data_for_PCoA(player_5)\n",
    "slice_data_for_PCoA(player_8)\n",
    "slice_data_for_PCoA(player_9)\n",
    "slice_data_for_PCoA(player_16)\n",
    "\n",
    "species = [i for i in species_master_dummy.columns if \"d__\" in i]\n",
    "sliced_df=sliced_df.loc[:,species]\n",
    "\n",
    "sliced_df[\"Slice\"] = np.repeat(range(1, 17), 7)\n",
    "print(sliced_df)\n",
    "sliced_df[\"BC_Dissimilarity\"] = np.nan\n",
    "sliced_df = sliced_df.groupby(\"Slice\").apply(lambda df: apply_BC(df))\n",
    "a = np.array([\"Day Pre\", \"Day of Impact\", \"1 day after\", \"2 days after\", \"3 days after\", \"4 days after\", \"5 days after\"])\n",
    "sliced_df[\"Day\"] = np.tile(a,16)\n",
    "sliced_df_BC = sliced_df.loc[:, [\"Slice\",\"BC_Dissimilarity\", \"Day\"]]\n",
    "sliced_df_BC = sliced_df_BC[sliced_df_BC[\"Day\"]!= \"Day Pre\"]\n",
    "sliced_df_BC = sliced_df_BC[sliced_df_BC[\"Day\"]!= \"5 days after\"]\n",
    "sns.boxplot(x = sliced_df_BC[\"Day\"], y = sliced_df_BC[\"BC_Dissimilarity\"]).figure.savefig(\"BC_3.png\", dpi = 300, bbox_inches = \"tight\")\n",
    "\n",
    "\n",
    "aovrm = AnovaRM(sliced_df_BC, depvar='BC_Dissimilarity', subject='Slice', within=['Day'],\n",
    "aggregate_func='mean'\n",
    ")\n",
    "res = aovrm.fit()\n",
    "print(res)\n",
    "\n",
    "b = sliced_df_BC.query('Day == \"1 day after\"')['BC_Dissimilarity']\n",
    "a = sliced_df_BC.query('Day == \"Day of Impact\"')['BC_Dissimilarity']\n",
    "c = sliced_df_BC.query('Day == \"2 days after\"')['BC_Dissimilarity']\n",
    "d = sliced_df_BC.query('Day == \"3 days after\"')['BC_Dissimilarity']\n",
    "e = sliced_df_BC.query('Day == \"4 days after\"')['BC_Dissimilarity']\n",
    "\n",
    "(stat, p) = stats.friedmanchisquare( a,b,c,d,e)\n",
    "print(stat)\n",
    "print(p)\n",
    "\n",
    "\n",
    "print(stats.ttest_rel(b,d))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.11 ('qiime2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a917a743856dcdb620580a044b4efb954a63c5f2a84ead791c39801b1461505"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
